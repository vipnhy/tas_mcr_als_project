#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TASæ•°æ®ç­›é€‰å™¨ - é’ˆå¯¹å®é™…æ•°æ®æ ¼å¼ä¼˜åŒ–ç‰ˆæœ¬
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from scipy import signal
from scipy.optimize import curve_fit
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class RobustTASScreener:
    """é’ˆå¯¹å®é™…TASæ•°æ®æ ¼å¼ä¼˜åŒ–çš„ç­›é€‰å™¨"""
    
    def __init__(self, output_root="experiments/results/data_screening"):
        self.output_root = Path(output_root)
        self.output_root.mkdir(parents=True, exist_ok=True)
        
        # æ‰«ædata/TASç›®å½•ä¸­çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
        self.known_files = self._scan_tas_directory()
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.results = {
            'multi_peak_overlap': [],
            'transient_decay': [],
            'low_snr': [],
            'analysis_summary': {}
        }
        
        # è¿˜å¯ä»¥æ£€æŸ¥resultsç›®å½•ä¸­çš„åˆæˆæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if Path("results/synthetic_tas_dataset.csv").exists():
            self.known_files.append("results/synthetic_tas_dataset.csv")
            
        # æ–°ç”Ÿæˆçš„æŒ‘æˆ˜æ€§æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        challenging_files = [
            self.output_root / "challenging_multi_peak_overlap.csv",
            self.output_root / "challenging_transient_decay.csv", 
            self.output_root / "challenging_low_snr.csv",
            self.output_root / "obvious_transient_decay.csv"
        ]
        
        for file in challenging_files:
            if Path(file).exists():
                self.known_files.append(file)
    
    def _generate_safe_filename(self, file_path, category):
        """ç”Ÿæˆå®‰å…¨çš„å¯è§†åŒ–æ–‡ä»¶åï¼ŒåŒ…å«ä¸Šçº§ç›®å½•ä¿¡æ¯å¹¶é¿å…é‡å"""
        file_path = Path(file_path)
        
        # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = file_path.stem
        
        # è·å–ä¸Šçº§ç›®å½•åï¼Œå¢åŠ å”¯ä¸€æ€§
        parent_dir = file_path.parent.name
        
        # æ„å»ºåŸºç¡€æ–‡ä»¶åï¼šä¸Šçº§ç›®å½•_åŸæ–‡ä»¶å
        safe_name = f"{parent_dir}_{base_name}"
        
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_name = safe_name.replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        category_dir = self.output_root / category
        category_dir.mkdir(exist_ok=True)
        
        final_path = category_dir / f"{safe_name}.png"
        
        # æ£€æŸ¥é‡åå¹¶å¤„ç†å†²çª
        counter = 1
        while final_path.exists():
            conflict_name = f"{safe_name}_{counter}"
            final_path = category_dir / f"{conflict_name}.png"
            counter += 1
            
        return final_path
    
    def _scan_tas_directory(self):
        """æ‰«ædata/TASç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶ï¼ŒæŒ‰ç›®å½•ç»„ç»‡å¹¶å®ç°ä¼˜å…ˆå¤„ç†é€»è¾‘"""
        tas_dir = Path("data/TAS")
        csv_files = []
        
        if not tas_dir.exists():
            print(f"âš ï¸ è­¦å‘Š: {tas_dir} ç›®å½•ä¸å­˜åœ¨")
            return csv_files
        
        # æŒ‰ç›®å½•ç»„ç»‡æ–‡ä»¶
        dir_files = {}
        for csv_file in tas_dir.rglob("*.csv"):
            dir_path = csv_file.parent
            if dir_path not in dir_files:
                dir_files[dir_path] = []
            dir_files[dir_path].append(csv_file)
        
        # å¯¹æ¯ä¸ªç›®å½•åº”ç”¨ä¼˜å…ˆå¤„ç†é€»è¾‘
        for dir_path, files in dir_files.items():
            file_names = [f.name for f in files]
            
            # å¦‚æœå­˜åœ¨TA_Average.csvï¼Œåªå¤„ç†å®ƒï¼Œè·³è¿‡TA_Scan*.csv
            if 'TA_Average.csv' in file_names:
                ta_average_file = dir_path / 'TA_Average.csv'
                csv_files.append(str(ta_average_file))
                print(f"ï¿½ {dir_path.name}: å‘ç°TA_Average.csvï¼Œè·³è¿‡TA_Scan*.csv")
            else:
                # å¦åˆ™å¤„ç†æ‰€æœ‰æ–‡ä»¶
                for file in files:
                    csv_files.append(str(file))
                print(f"ğŸ“ {dir_path.name}: å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")
        
        print(f"ğŸ“„ æ€»å…±é€‰æ‹©å¤„ç† {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        return csv_files
    
    def load_tas_file(self, file_path):
        """åŠ è½½TASæ–‡ä»¶ - å¥å£®ç‰ˆæœ¬"""
        try:
            print(f"   æ­£åœ¨åŠ è½½: {Path(file_path).name}")
            
            # é¦–å…ˆå°è¯•è¯»å–åŸå§‹CSV
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            # è·³è¿‡å¯èƒ½çš„å¤´éƒ¨ä¿¡æ¯
            data_start = 0
            for i, line in enumerate(lines):
                if line.strip() and ',' in line:
                    try:
                        # å°è¯•è§£æç¬¬ä¸€è¡Œä¸ºæ•°å­—
                        float(line.split(',')[0])
                        data_start = i
                        break
                    except:
                        continue
            
            if data_start >= len(lines) - 5:
                print("   âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆæ•°æ®è¡Œ")
                return None
            
            # è¯»å–æ•°æ®éƒ¨åˆ†
            data_lines = lines[data_start:]
            
            # è§£æç¬¬ä¸€è¡Œä¸ºæ—¶é—´å»¶è¿Ÿ
            time_delays = []
            first_line = data_lines[0].strip().split(',')
            for val in first_line:
                try:
                    time_delays.append(float(val))
                except:
                    time_delays.append(0.0)
            
            time_delays = np.array(time_delays)
            
            # è§£æå…¶ä½™è¡Œ
            wavelengths = []
            data_matrix = []
            
            for line in data_lines[1:]:
                if not line.strip():
                    continue
                    
                parts = line.strip().split(',')
                if len(parts) < 2:
                    continue
                
                try:
                    # ç¬¬ä¸€ä¸ªå€¼æ˜¯æ³¢é•¿
                    wl = float(parts[0])
                    wavelengths.append(wl)
                    
                    # å…¶ä½™å€¼æ˜¯æ•°æ®
                    row_data = []
                    for val in parts[1:]:
                        try:
                            v = float(val)
                            if np.isfinite(v):
                                row_data.append(v)
                            else:
                                row_data.append(0.0)
                        except:
                            row_data.append(0.0)
                    
                    # ç¡®ä¿é•¿åº¦åŒ¹é…
                    while len(row_data) < len(time_delays):
                        row_data.append(0.0)
                    row_data = row_data[:len(time_delays)]
                    
                    data_matrix.append(row_data)
                    
                except Exception as e:
                    print(f"   âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                    continue
            
            if len(data_matrix) < 10:
                print(f"   âŒ æ•°æ®è¡Œæ•°å¤ªå°‘: {len(data_matrix)}")
                return None
            
            wavelengths = np.array(wavelengths)
            data = np.array(data_matrix)
            
            # æ•°æ®éªŒè¯å’Œæ¸…ç†
            if data.shape[0] < 10 or data.shape[1] < 10:
                print(f"   âŒ æ•°æ®å½¢çŠ¶å¤ªå°: {data.shape}")
                return None
            
            # å¤„ç†å¼‚å¸¸å€¼å’Œæ— ç©·å¤§
            data = np.where(np.isfinite(data), data, 0)
            
            # ç§»é™¤å…¨é›¶è¡Œå’Œåˆ—
            non_zero_rows = np.any(np.abs(data) > 1e-10, axis=1)
            non_zero_cols = np.any(np.abs(data) > 1e-10, axis=0)
            
            if np.sum(non_zero_rows) < 5 or np.sum(non_zero_cols) < 5:
                print("   âŒ æœ‰æ•ˆæ•°æ®å¤ªå°‘")
                return None
            
            data = data[non_zero_rows, :][:, non_zero_cols]
            wavelengths = wavelengths[non_zero_rows]
            time_delays = time_delays[non_zero_cols]
            
            print(f"   âœ… æˆåŠŸåŠ è½½: {data.shape} (æ³¢é•¿Ã—æ—¶é—´)")
            print(f"   æ³¢é•¿èŒƒå›´: {wavelengths.min():.1f} - {wavelengths.max():.1f} nm")
            print(f"   æ—¶é—´èŒƒå›´: {time_delays.min():.2f} - {time_delays.max():.2f} ps")
            print(f"   æ•°æ®èŒƒå›´: {data.min():.2e} - {data.max():.2e}")
            
            # æ ¹æ®å…ˆéªŒçŸ¥è¯†è¿›è¡Œå…‰è°±è£å‰ª
            file_path_str = str(file_path)
            original_wavelength_range = f"{wavelengths.min():.1f} - {wavelengths.max():.1f} nm"
            
            # åˆ¤æ–­å…‰è°±ç±»å‹å¹¶ç¡®å®šè£å‰ªèŒƒå›´
            if 'UV' in file_path_str.upper():
                # UVå…‰è°±: 380-650 nm
                wl_min, wl_max = 380.0, 650.0
                spectrum_type = "UV"
            elif 'NIR' in file_path_str.upper():
                # NIRå…‰è°±: 1100-1620 nm
                wl_min, wl_max = 1100.0, 1620.0
                spectrum_type = "NIR"
            else:
                # é»˜è®¤VISå…‰è°±: 500-950 nm
                wl_min, wl_max = 500.0, 950.0
                spectrum_type = "VIS"
            
            # æ‰§è¡Œå…‰è°±è£å‰ª
            mask = (wavelengths >= wl_min) & (wavelengths <= wl_max)
            if np.sum(mask) < 5:
                print(f"   âš ï¸ è£å‰ªåæœ‰æ•ˆæ³¢é•¿ç‚¹å¤ªå°‘ ({np.sum(mask)}), è·³è¿‡æ­¤æ–‡ä»¶")
                return None
            
            wavelengths_cropped = wavelengths[mask]
            data_cropped = data[mask, :]
            
            print(f"   âœ‚ï¸ å…‰è°±è£å‰ª: {spectrum_type}èŒƒå›´ ({wl_min:.0f}-{wl_max:.0f} nm)")
            print(f"   è£å‰ªå‰: {original_wavelength_range}")
            print(f"   è£å‰ªå: {wavelengths_cropped.min():.1f} - {wavelengths_cropped.max():.1f} nm")
            print(f"   ä¿ç•™æ³¢é•¿ç‚¹: {len(wavelengths_cropped)}/{len(wavelengths)}")
            
            return {
                'data': data_cropped,
                'wavelengths': wavelengths_cropped,
                'time_delays': time_delays,
                'shape': data_cropped.shape,
                'file_path': str(file_path),
                'spectrum_type': spectrum_type,
                'original_wavelength_range': original_wavelength_range,
                'cropped_wavelength_range': f"{wavelengths_cropped.min():.1f} - {wavelengths_cropped.max():.1f} nm"
            }
            
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def analyze_multi_peak_overlap(self, data_info):
        """åˆ†æå¤šå³°é‡å ç‰¹å¾"""
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        
        overlap_scores = []
        
        # å¯»æ‰¾æœ‰æ˜æ˜¾ä¿¡å·çš„æ—¶é—´ç‚¹
        signal_strength = np.max(np.abs(data), axis=0)
        strong_signal_indices = np.argsort(signal_strength)[-min(10, len(signal_strength)):]
        
        for t_idx in strong_signal_indices:
            spectrum = data[:, t_idx]
            abs_spectrum = np.abs(spectrum)
            
            if np.max(abs_spectrum) < 1e-8:
                continue
            
            # å¯»æ‰¾å³°å€¼
            try:
                # è‡ªé€‚åº”é˜ˆå€¼
                prominence = np.std(abs_spectrum) * 0.3
                min_distance = max(3, len(abs_spectrum) // 30)
                
                peaks, properties = signal.find_peaks(
                    abs_spectrum,
                    prominence=prominence,
                    distance=min_distance,
                    height=np.max(abs_spectrum) * 0.1
                )
                
                if len(peaks) >= 3:  # è‡³å°‘3ä¸ªå³°
                    # è®¡ç®—å³°é—´é‡å åº¦
                    try:
                        peak_widths = signal.peak_widths(abs_spectrum, peaks, rel_height=0.5)[0]
                        peak_distances = np.diff(peaks)
                        
                        if len(peak_distances) > 0:
                            avg_width = np.mean(peak_widths[:-1])
                            avg_distance = np.mean(peak_distances)
                            overlap_ratio = avg_width / avg_distance if avg_distance > 0 else 0
                            
                            if overlap_ratio > 0.3:  # é™ä½é‡å é˜ˆå€¼
                                overlap_scores.append({
                                    'time_index': t_idx,
                                    'time_delay': data_info['time_delays'][t_idx],
                                    'num_peaks': len(peaks),
                                    'overlap_ratio': overlap_ratio,
                                    'peaks': peaks.tolist(),
                                    'peak_heights': abs_spectrum[peaks].tolist(),
                                    'peak_widths': peak_widths.tolist()
                                })
                    except:
                        continue
            except:
                continue
        
        if overlap_scores:
            avg_overlap = np.mean([s['overlap_ratio'] for s in overlap_scores])
            max_peaks = max([s['num_peaks'] for s in overlap_scores])
            
            return {
                'is_multi_peak': True,
                'score': avg_overlap,
                'max_peaks': max_peaks,
                'num_overlap_times': len(overlap_scores),
                'details': overlap_scores[:3]  # ä¿ç•™å‰3ä¸ªæœ€å¥½çš„
            }
        
        return {'is_multi_peak': False, 'score': 0, 'details': []}
    
    def analyze_transient_decay(self, data_info):
        """åˆ†æç¬æ€è¡°å‡ç‰¹å¾"""
        data = data_info['data']
        time_delays = data_info['time_delays']
        
        # åªåˆ†ææ­£æ—¶é—´å»¶è¿Ÿ
        pos_mask = time_delays > 0
        if np.sum(pos_mask) < 5:
            return {'is_transient': False, 'score': 0, 'details': []}
        
        pos_times = time_delays[pos_mask]
        pos_data = data[:, pos_mask]
        
        decay_results = []
        
        # åˆ†ææœ‰å¼ºä¿¡å·çš„æ³¢é•¿
        max_signal_per_wavelength = np.max(np.abs(pos_data), axis=1)
        if np.max(max_signal_per_wavelength) < 1e-8:
            return {'is_transient': False, 'score': 0, 'details': []}
        
        strong_wl_indices = np.argsort(max_signal_per_wavelength)[-min(15, len(max_signal_per_wavelength)):]
        
        for wl_idx in strong_wl_indices:
            kinetic = pos_data[wl_idx, :]
            
            if len(kinetic) < 5:
                continue
            
            # æ£€æŸ¥è¡°å‡è¶‹åŠ¿
            initial_vals = kinetic[:min(3, len(kinetic))]
            final_vals = kinetic[-min(3, len(kinetic)):]
            
            initial = np.mean(np.abs(initial_vals))
            final = np.mean(np.abs(final_vals))
            
            if initial < 1e-10:
                continue
            
            decay_ratio = initial / final if final > 1e-10 else 100
            
            if decay_ratio > 1.1:  # è¿›ä¸€æ­¥é™ä½è¡°å‡é˜ˆå€¼
                try:
                    # ç®€å•çš„æŒ‡æ•°è¡°å‡æ‹Ÿåˆ
                    def exp_decay(t, a, tau, c):
                        return a * np.exp(-t / tau) + c
                    
                    # æ›´å¥½çš„åˆå§‹çŒœæµ‹
                    y_data = kinetic
                    max_val = np.max(np.abs(y_data))
                    min_val = np.min(np.abs(y_data))
                    
                    initial_a = max_val - min_val
                    initial_tau = pos_times[len(pos_times)//3]
                    initial_c = min_val
                    
                    popt, pcov = curve_fit(
                        exp_decay,
                        pos_times,
                        y_data,
                        p0=[initial_a, initial_tau, initial_c],
                        bounds=([-np.inf, 0.01, -np.inf], [np.inf, 1000, np.inf]),
                        maxfev=1000
                    )
                    
                    # è®¡ç®—æ‹Ÿåˆè´¨é‡
                    fitted = exp_decay(pos_times, *popt)
                    ss_res = np.sum((y_data - fitted)**2)
                    ss_tot = np.sum((y_data - np.mean(y_data))**2)
                    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
                    
                    if r2 > 0.3 and 0.1 <= abs(popt[1]) <= 200:  # è¿›ä¸€æ­¥é™ä½RÂ²é˜ˆå€¼ï¼Œæ‰©å¤§æ—¶é—´å¸¸æ•°èŒƒå›´
                        decay_results.append({
                            'wavelength_index': int(wl_idx),
                            'wavelength': float(data_info['wavelengths'][wl_idx]),
                            'time_constant': abs(float(popt[1])),
                            'r2': float(r2),
                            'decay_ratio': float(decay_ratio),
                            'amplitude': float(popt[0]),
                            'offset': float(popt[2])
                        })
                
                except:
                    continue
        
        if decay_results:
            avg_r2 = np.mean([r['r2'] for r in decay_results])
            avg_tau = np.mean([r['time_constant'] for r in decay_results])
            
            return {
                'is_transient': True,
                'score': avg_r2,
                'avg_time_constant': avg_tau,
                'num_decay_wavelengths': len(decay_results),
                'details': decay_results[:3]  # ä¿ç•™å‰3ä¸ªæœ€å¥½çš„
            }
        
        return {'is_transient': False, 'score': 0, 'details': []}
    
    def analyze_low_snr(self, data_info):
        """åˆ†æä¿¡å™ªæ¯”"""
        data = data_info['data']
        
        # æ›´ä¿å®ˆçš„å™ªå£°ä¼°è®¡
        h, w = data.shape
        
        # ä½¿ç”¨è¾¹ç¼˜åŒºåŸŸä¼°è®¡å™ªå£°
        edge_size = max(2, min(h//10, w//10, 5))
        
        try:
            # å››ä¸ªè§’çš„æ•°æ®
            corners = [
                data[:edge_size, :edge_size],
                data[:edge_size, -edge_size:],
                data[-edge_size:, :edge_size],
                data[-edge_size:, -edge_size:]
            ]
            
            noise_stds = []
            for corner in corners:
                if corner.size > 0:
                    corner_flat = corner.flatten()
                    # ç§»é™¤å¼‚å¸¸å€¼åè®¡ç®—æ ‡å‡†å·®
                    percentiles = np.percentile(corner_flat, [25, 75])
                    iqr = percentiles[1] - percentiles[0]
                    if iqr > 0:
                        mask = (corner_flat >= percentiles[0] - 1.5*iqr) & \
                               (corner_flat <= percentiles[1] + 1.5*iqr)
                        if np.sum(mask) > 0:
                            noise_stds.append(np.std(corner_flat[mask]))
            
            noise_level = np.mean(noise_stds) if noise_stds else np.std(data) * 0.1
            
            # ä¼°è®¡ä¿¡å·å¼ºåº¦ï¼ˆä¸­å¿ƒåŒºåŸŸæœ€å¤§å€¼ï¼‰
            center_h = slice(h//6, 5*h//6)
            center_w = slice(w//6, 5*w//6)
            center_data = data[center_h, center_w]
            
            if center_data.size > 0:
                signal_strength = np.max(np.abs(center_data))
            else:
                signal_strength = np.max(np.abs(data))
            
            # è®¡ç®—SNR
            snr = signal_strength / noise_level if noise_level > 0 else float('inf')
            
            return {
                'is_low_snr': snr <= 10.0,  # è°ƒæ•´SNRé˜ˆå€¼
                'snr': float(snr),
                'noise_level': float(noise_level),
                'signal_strength': float(signal_strength)
            }
            
        except Exception as e:
            print(f"   âš ï¸ SNRåˆ†æå¤±è´¥: {e}")
            return {
                'is_low_snr': False,
                'snr': float('inf'),
                'noise_level': 0.0,
                'signal_strength': float(np.max(np.abs(data)))
            }
    
    def create_comprehensive_visualization(self, data_info, analyses, output_file):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        time_delays = data_info['time_delays']
        file_name = Path(data_info['file_path']).name
        
        fig = plt.figure(figsize=(20, 15))
        
        # 1. ä¸»è¦2Dçƒ­å›¾
        ax1 = plt.subplot(3, 4, 1)
        im = ax1.imshow(data.T, aspect='auto', cmap='rainbow',
                       extent=[wavelengths.min(), wavelengths.max(),
                              time_delays.max(), time_delays.min()])
        ax1.set_xlabel('æ³¢é•¿ (nm)')
        ax1.set_ylabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax1.set_title(f'TAS 2D å…‰è°±çƒ­å›¾\n{file_name}', fontweight='bold')
        plt.colorbar(im, ax=ax1, label='Î”A')
        
        # 2. æ—¶é—´åˆ‡ç‰‡å…‰è°±ï¼ˆå¤šå³°é‡å åˆ†æï¼‰
        ax2 = plt.subplot(3, 4, 2)
        if analyses['multi_peak']['is_multi_peak'] and analyses['multi_peak']['details']:
            detail = analyses['multi_peak']['details'][0]
            t_idx = detail['time_index']
            spectrum = data[:, t_idx]
            peaks = detail['peaks']
            
            ax2.plot(wavelengths, np.abs(spectrum), 'b-', linewidth=2, label='å…‰è°±')
            if len(peaks) > 0:
                ax2.plot(wavelengths[peaks], np.abs(spectrum[peaks]), 'ro', 
                        markersize=8, label=f'å³° (n={len(peaks)})')
            ax2.set_title(f'å¤šå³°è¯†åˆ«\nt={time_delays[t_idx]:.2f}ps\né‡å åº¦:{detail["overlap_ratio"]:.3f}')
        else:
            # æ˜¾ç¤ºä¿¡å·æœ€å¼ºçš„æ—¶é—´åˆ‡ç‰‡
            signal_strength = np.max(np.abs(data), axis=0)
            max_t_idx = np.argmax(signal_strength)
            spectrum = data[:, max_t_idx]
            ax2.plot(wavelengths, np.abs(spectrum), 'b-', linewidth=2)
            ax2.set_title(f'å…‰è°±åˆ‡ç‰‡\nt={time_delays[max_t_idx]:.2f}ps')
        
        ax2.set_xlabel('æ³¢é•¿ (nm)')
        ax2.set_ylabel('|Î”A|')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. åŠ¨åŠ›å­¦æ›²çº¿ï¼ˆç¬æ€è¡°å‡åˆ†æï¼‰
        ax3 = plt.subplot(3, 4, 3)
        pos_mask = time_delays > 0
        if analyses['transient']['is_transient'] and analyses['transient']['details'] and np.sum(pos_mask) > 0:
            detail = analyses['transient']['details'][0]
            wl_idx = detail['wavelength_index']
            
            pos_times = time_delays[pos_mask]
            pos_data = data[wl_idx, pos_mask]
            
            ax3.plot(pos_times, pos_data, 'bo', markersize=4, label='å®éªŒæ•°æ®')
            
            # æ‹Ÿåˆæ›²çº¿
            try:
                tau = detail['time_constant']
                amp = detail['amplitude']
                offset = detail['offset']
                fitted = amp * np.exp(-pos_times / tau) + offset
                ax3.plot(pos_times, fitted, 'r-', linewidth=2,
                        label=f'æ‹Ÿåˆ (Ï„={tau:.2f}ps)')
            except:
                pass
            
            ax3.set_title(f'ç¬æ€è¡°å‡\nÎ»={wavelengths[wl_idx]:.0f}nm\nRÂ²={detail["r2"]:.3f}')
        else:
            # æ˜¾ç¤ºä¿¡å·æœ€å¼ºçš„æ³¢é•¿åŠ¨åŠ›å­¦
            signal_per_wl = np.max(np.abs(data), axis=1)
            max_wl_idx = np.argmax(signal_per_wl)
            kinetic = data[max_wl_idx, :]
            ax3.plot(time_delays, kinetic, 'b-', linewidth=2)
            ax3.set_title(f'åŠ¨åŠ›å­¦æ›²çº¿\nÎ»={wavelengths[max_wl_idx]:.0f}nm')
        
        ax3.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax3.set_ylabel('Î”A')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ä¿¡å™ªæ¯”åˆ†æ
        ax4 = plt.subplot(3, 4, 4)
        snr = analyses['low_snr']['snr']
        data_flat = data.flatten()
        
        # ç§»é™¤å¼‚å¸¸å€¼ç»˜åˆ¶ç›´æ–¹å›¾
        percentiles = np.percentile(data_flat, [1, 99])
        mask = (data_flat >= percentiles[0]) & (data_flat <= percentiles[1])
        clean_data = data_flat[mask]
        
        ax4.hist(clean_data, bins=50, alpha=0.7, density=True, color='skyblue')
        ax4.axvline(0, color='red', linestyle='--', alpha=0.7, label='é›¶åŸºçº¿')
        ax4.set_title(f'æ•°æ®åˆ†å¸ƒ\nSNR = {snr:.2f}')
        ax4.set_xlabel('ä¿¡å·å¼ºåº¦')
        ax4.set_ylabel('å¯†åº¦')
        ax4.legend()
        
        # 5-8. ä¸åŒæ—¶é—´ç‚¹çš„å…‰è°±
        n_times = len(time_delays)
        time_indices = [0, n_times//4, n_times//2, 3*n_times//4]
        colors = ['red', 'blue', 'green', 'orange']
        
        for i, (t_idx, color) in enumerate(zip(time_indices, colors)):
            ax = plt.subplot(3, 4, 5+i)
            spectrum = data[:, t_idx]
            ax.plot(wavelengths, spectrum, color=color, linewidth=2)
            ax.set_title(f't = {time_delays[t_idx]:.2f} ps')
            ax.set_xlabel('æ³¢é•¿ (nm)')
            ax.set_ylabel('Î”A')
            ax.grid(True, alpha=0.3)
        
        # 9-12. ä¸åŒæ³¢é•¿çš„åŠ¨åŠ›å­¦
        n_wls = len(wavelengths)
        wl_indices = [n_wls//6, n_wls//3, 2*n_wls//3, 5*n_wls//6]
        
        for i, wl_idx in enumerate(wl_indices):
            ax = plt.subplot(3, 4, 9+i)
            kinetic = data[wl_idx, :]
            ax.plot(time_delays, kinetic, linewidth=2, color=colors[i])
            ax.set_title(f'Î» = {wavelengths[wl_idx]:.0f} nm')
            ax.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
            ax.set_ylabel('Î”A')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # å°è¯•ä¿å­˜å¯è§†åŒ–ï¼Œå¤„ç†PILé”™è¯¯
        try:
            plt.savefig(output_file, dpi=150, bbox_inches='tight')
            try:
                relative_path = output_file.relative_to(self.output_root)
                print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜: {relative_path}")
            except ValueError:
                print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜: {output_file}")
        except Exception as e:
            print(f"     âš ï¸ é«˜è´¨é‡ä¿å­˜å¤±è´¥ï¼Œå°è¯•ä½è´¨é‡ä¿å­˜: {e}")
            try:
                # é™ä½DPIå¹¶ç§»é™¤bbox_incheså‚æ•°
                plt.savefig(output_file, dpi=100)
                try:
                    relative_path = output_file.relative_to(self.output_root)
                    print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜(ä½è´¨é‡): {relative_path}")
                except ValueError:
                    print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜(ä½è´¨é‡): {output_file}")
            except Exception as e2:
                print(f"     âŒ å¯è§†åŒ–ä¿å­˜å¤±è´¥: {e2}")
                # å°è¯•ä¿å­˜ä¸ºPDFæ ¼å¼
                try:
                    pdf_file = output_file.with_suffix('.pdf')
                    plt.savefig(pdf_file, format='pdf')
                    try:
                        relative_path = pdf_file.relative_to(self.output_root)
                        print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜ä¸ºPDF: {relative_path}")
                    except ValueError:
                        print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜ä¸ºPDF: {pdf_file}")
                except Exception as e3:
                    print(f"     âŒ PDFä¿å­˜ä¹Ÿå¤±è´¥: {e3}")
        
        plt.close()
    
    def run_screening(self):
        """è¿è¡Œç­›é€‰"""
        print("ğŸ” å¼€å§‹ç­›é€‰TASæŒ‘æˆ˜æ€§æ•°æ®")
        print("=" * 50)
        
        # æŒ‰ç›®å½•åˆ†ç»„æ–‡ä»¶ï¼Œä¼˜å…ˆå¤„ç†TA_Average.csv
        files_by_dir = {}
        for file_path in self.known_files:
            if not Path(file_path).exists():
                continue
            dir_path = str(Path(file_path).parent)
            if dir_path not in files_by_dir:
                files_by_dir[dir_path] = []
            files_by_dir[dir_path].append(file_path)
        
        # å¯¹æ¯ä¸ªç›®å½•ï¼Œä¼˜å…ˆå¤„ç†TA_Average.csvï¼Œè·³è¿‡TA_Scan*.csv
        processed_files = []
        for dir_path, files in files_by_dir.items():
            avg_files = [f for f in files if Path(f).name == 'TA_Average.csv']
            scan_files = [f for f in files if Path(f).name.startswith('TA_Scan') and Path(f).name.endswith('.csv')]
            other_files = [f for f in files if f not in avg_files and f not in scan_files]
            
            # å¦‚æœå­˜åœ¨TA_Average.csvï¼Œè·³è¿‡TA_Scan*.csvæ–‡ä»¶
            if avg_files:
                processed_files.extend(avg_files)
                print(f"ğŸ“ ç›®å½• {Path(dir_path).name}: å‘ç°TA_Average.csvï¼Œè·³è¿‡TA_Scanæ–‡ä»¶")
            else:
                processed_files.extend(scan_files)
            
            # æ€»æ˜¯å¤„ç†å…¶ä»–æ–‡ä»¶
            processed_files.extend(other_files)
        
        for file_path in processed_files:
            if not Path(file_path).exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {Path(file_path).name}")
            
            # åŠ è½½æ•°æ®
            data_info = self.load_tas_file(file_path)
            if data_info is None:
                continue
            
            # åˆ†æä¸‰ç±»ç‰¹å¾
            print("   ğŸ”¬ æ­£åœ¨åˆ†æå¤šå³°é‡å ...")
            multi_peak = self.analyze_multi_peak_overlap(data_info)
            
            print("   âš¡ æ­£åœ¨åˆ†æç¬æ€è¡°å‡...")
            transient = self.analyze_transient_decay(data_info)
            
            print("   ğŸ“Š æ­£åœ¨åˆ†æä¿¡å™ªæ¯”...")
            low_snr = self.analyze_low_snr(data_info)
            
            analyses = {
                'multi_peak': multi_peak,
                'transient': transient,
                'low_snr': low_snr
            }
            
            # åˆ†ç±»ç»“æœ
            categories = []
            
            print("   ğŸ“‹ åˆ†æç»“æœ:")
            if multi_peak['is_multi_peak']:
                categories.append('multi_peak_overlap')
                print(f"     âœ… å¤šå³°é‡å : è¯„åˆ†={multi_peak['score']:.3f}, å³°æ•°={multi_peak['max_peaks']}")
            else:
                print(f"     â– å¤šå³°é‡å : æœªæ£€æµ‹åˆ°")
            
            if transient['is_transient']:
                categories.append('transient_decay')
                print(f"     âœ… ç¬æ€è¡°å‡: RÂ²={transient['score']:.3f}, Ï„å¹³å‡={transient['avg_time_constant']:.2f}ps")
            else:
                print(f"     â– ç¬æ€è¡°å‡: æœªæ£€æµ‹åˆ°")
            
            if low_snr['is_low_snr']:
                categories.append('low_snr')
                print(f"     âœ… ä½ä¿¡å™ªæ¯”: SNR={low_snr['snr']:.2f}")
            else:
                print(f"     â– ä¿¡å™ªæ¯”è‰¯å¥½: SNR={low_snr['snr']:.2f}")
            
            # ä¿å­˜ç»“æœ
            file_result = {
                'file_name': Path(file_path).name,
                'file_path': str(Path(file_path).resolve()),  # ç»å¯¹è·¯å¾„
                'relative_path': str(file_path),  # ç›¸å¯¹è·¯å¾„
                'shape': data_info['shape'],
                'wavelength_range': [float(data_info['wavelengths'].min()), float(data_info['wavelengths'].max())],
                'time_range': [float(data_info['time_delays'].min()), float(data_info['time_delays'].max())],
                'categories': categories,
                'analyses': self._serialize_analyses(analyses),
                # æ·»åŠ å…‰è°±ç±»å‹ç›¸å…³ä¿¡æ¯
                'spectrum_type': data_info.get('spectrum_type', 'VIS'),
                'original_wavelength_range': data_info.get('original_wavelength_range', 'N/A'),
                'cropped_wavelength_range': data_info.get('cropped_wavelength_range', 'N/A')
            }
            
            # ä¸ºæ¯ä¸ªç±»åˆ«ä¿å­˜
            if categories:
                for category in categories:
                    self.results[category].append(file_result)
                    
                    # ç”Ÿæˆå®‰å…¨çš„å¯è§†åŒ–æ–‡ä»¶å
                    viz_file = self._generate_safe_filename(file_path, category)
                    
                    try:
                        print(f"   ğŸ¨ ç”Ÿæˆ{category}å¯è§†åŒ–...")
                        self.create_comprehensive_visualization(data_info, analyses, viz_file)
                        try:
                            relative_path = viz_file.relative_to(Path.cwd())
                            print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜: {relative_path}")
                        except ValueError:
                            print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜: {viz_file}")
                    except Exception as e:
                        print(f"     âŒ å¯è§†åŒ–å¤±è´¥: {e}")
            else:
                print("   ğŸ“ è¯¥æ–‡ä»¶ä¸å±äºæŒ‘æˆ˜æ€§æ•°æ®ç±»åˆ«")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        self.save_results()
    
    def _serialize_analyses(self, analyses):
        """åºåˆ—åŒ–åˆ†æç»“æœä»¥ä¾¿JSONä¿å­˜"""
        def convert(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, (bool, np.bool_)):
                return bool(obj)
            elif isinstance(obj, dict):
                return {k: convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert(item) for item in obj]
            return obj
        
        return convert(analyses)
    
    def generate_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report_file = self.output_root / "TASæ•°æ®ç­›é€‰æŠ¥å‘Š.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# TASæ•°æ®ç­›é€‰æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡
            f.write("## ğŸ“Š ç­›é€‰ç»Ÿè®¡\n\n")
            f.write(f"- **å¤šå³°é‡å æ•°æ®**: {len(self.results['multi_peak_overlap'])} ä¸ª\n")
            f.write(f"- **ç¬æ€è¡°å‡æ•°æ®**: {len(self.results['transient_decay'])} ä¸ª\n")
            f.write(f"- **ä½ä¿¡å™ªæ¯”æ•°æ®**: {len(self.results['low_snr'])} ä¸ª\n\n")
            
            # è¯¦ç»†ç»“æœ
            for category, title in [
                ('multi_peak_overlap', 'å¤šå³°é‡å æ•°æ®'),
                ('transient_decay', 'ç¬æ€è¡°å‡æ•°æ®'),
                ('low_snr', 'ä½ä¿¡å™ªæ¯”æ•°æ®')
            ]:
                f.write(f"## ğŸ¯ {title}\n\n")
                
                if self.results[category]:
                    for i, item in enumerate(self.results[category], 1):
                        f.write(f"### {i}. {item['file_name']}\n\n")
                        f.write(f"- **æ–‡ä»¶è·¯å¾„**: `{Path(item['file_path']).resolve()}`\n")
                        f.write(f"- **ç›¸å¯¹è·¯å¾„**: `{item.get('relative_path', item['file_path'])}`\n")
                        f.write(f"- **å…‰è°±ç±»å‹**: {item.get('spectrum_type', 'VIS')}\n")
                        f.write(f"- **åŸå§‹æ³¢é•¿èŒƒå›´**: {item.get('original_wavelength_range', 'N/A')}\n")
                        f.write(f"- **è£å‰ªåæ³¢é•¿èŒƒå›´**: {item.get('cropped_wavelength_range', item['wavelength_range'])}\n")
                        f.write(f"- **æ•°æ®å½¢çŠ¶**: {item['shape'][0]}Ã—{item['shape'][1]} (æ³¢é•¿Ã—æ—¶é—´)\n")
                        f.write(f"- **æ—¶é—´èŒƒå›´**: {item['time_range'][0]:.2f} - {item['time_range'][1]:.2f} ps\n")
                        
                        # ç‰¹å¾åˆ†æç»“æœ
                        analyses = item['analyses']
                        if category == 'multi_peak_overlap':
                            mp = analyses['multi_peak']
                            f.write(f"- **é‡å è¯„åˆ†**: {mp['score']:.3f}\n")
                            f.write(f"- **æœ€å¤§å³°æ•°**: {mp['max_peaks']}\n")
                            f.write(f"- **é‡å æ—¶é—´ç‚¹æ•°**: {mp['num_overlap_times']}\n")
                        elif category == 'transient_decay':
                            td = analyses['transient']
                            f.write(f"- **å¹³å‡æ‹ŸåˆRÂ²**: {td['score']:.3f}\n")
                            f.write(f"- **å¹³å‡æ—¶é—´å¸¸æ•°**: {td['avg_time_constant']:.2f} ps\n")
                            f.write(f"- **è¡°å‡æ³¢é•¿æ•°**: {td['num_decay_wavelengths']}\n")
                        elif category == 'low_snr':
                            ls = analyses['low_snr']
                            f.write(f"- **ä¿¡å™ªæ¯”**: {ls['snr']:.2f}\n")
                            f.write(f"- **å™ªå£°æ°´å¹³**: {ls['noise_level']:.2e}\n")
                            f.write(f"- **ä¿¡å·å¼ºåº¦**: {ls['signal_strength']:.2e}\n")
                        
                        # å¯è§†åŒ–æ–‡ä»¶
                        parent_dir = Path(item['file_path']).parent.name
                        safe_name = f"{parent_dir}_{Path(item['file_path']).stem}".replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
                        viz_file = f"{category}/{safe_name}.png"
                        f.write(f"- **å¯è§†åŒ–æ–‡ä»¶**: {viz_file}\n\n")
                        f.write(f"![{title}å¯è§†åŒ–]({viz_file})\n\n")
                else:
                    f.write("æš‚æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®ã€‚\n\n")
            
            # ä½¿ç”¨å»ºè®®
            f.write("## ğŸ’¡ ä½¿ç”¨å»ºè®®\n\n")
            f.write("### å¤šå³°é‡å æ•°æ®\n")
            f.write("- æµ‹è¯•MCR-ALSåœ¨å¤æ‚å…‰è°±é‡å ä¸‹çš„ç»„åˆ†åˆ†è¾¨èƒ½åŠ›\n")
            f.write("- å»ºè®®ä½¿ç”¨æ›´å¤šç»„åˆ†æ•°å’Œä¸¥æ ¼çº¦æŸ\n")
            f.write("- å¯å°è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•\n\n")
            
            f.write("### ç¬æ€è¡°å‡æ•°æ®\n")
            f.write("- éªŒè¯MCR-ALSå¯¹æ—¶é—´åˆ†è¾¨åŠ¨åŠ›å­¦çš„åˆ†æèƒ½åŠ›\n")
            f.write("- å…³æ³¨æ—¶é—´å¸¸æ•°çš„ç‰©ç†åˆç†æ€§\n")
            f.write("- å¯ç»“åˆå·²çŸ¥åŠ¨åŠ›å­¦æ¨¡å‹éªŒè¯\n\n")
            
            f.write("### ä½ä¿¡å™ªæ¯”æ•°æ®\n")
            f.write("- æµ‹è¯•ç®—æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§\n")
            f.write("- å»ºè®®å¢åŠ éšæœºåˆå§‹åŒ–æ¬¡æ•°\n")
            f.write("- å¯å°è¯•é¢„å¤„ç†é™å™ªæŠ€æœ¯\n\n")
        
        print(f"\nğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_file = self.output_root / "screening_results.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ TASæŒ‘æˆ˜æ€§æ•°æ®ç­›é€‰å™¨ - å¥å£®ç‰ˆ")
    print("=" * 50)
    
    screener = RobustTASScreener()
    screener.run_screening()
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ğŸ‰ ç­›é€‰å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {screener.output_root}")
    print(f"ğŸ¯ å¤šå³°é‡å æ•°æ®: {len(screener.results['multi_peak_overlap'])} ä¸ª")
    print(f"âš¡ ç¬æ€è¡°å‡æ•°æ®: {len(screener.results['transient_decay'])} ä¸ª")
    print(f"ğŸ“‰ ä½ä¿¡å™ªæ¯”æ•°æ®: {len(screener.results['low_snr'])} ä¸ª")
    
    # æ˜¾ç¤ºè¾“å‡ºç›®å½•ç»“æ„
    print(f"\nğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
    for item in screener.output_root.iterdir():
        if item.is_dir():
            file_count = len(list(item.glob('*.png')))
            print(f"  ğŸ“ {item.name}/ ({file_count} ä¸ªå¯è§†åŒ–æ–‡ä»¶)")
        else:
            print(f"  ğŸ“„ {item.name}")

if __name__ == "__main__":
    main()