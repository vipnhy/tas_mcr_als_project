#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TASæ•°æ®ç­›é€‰å™¨ - ä»data/TASç›®å½•ç­›é€‰ä¸‰ç±»æŒ‘æˆ˜æ€§æ•°æ®
1. å¤šå³°é‡å æ•°æ®ï¼ˆæ¨¡æ‹Ÿå¤æ‚ä½“ç³»ï¼‰
2. ç¬æ€ä¿¡å·è¡°å‡æ•°æ®ï¼ˆæ—¶é—´åˆ†è¾¨éªŒè¯ï¼‰
3. ä½ä¿¡å™ªæ¯”æ•°æ®ï¼ˆSNR=5:1ï¼‰
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from scipy import signal
from scipy.optimize import curve_fit
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class TASDataScreener:
    """TASæ•°æ®ç­›é€‰å™¨"""
    
    def __init__(self, data_root="data/TAS", output_root="experiments/results/data_screening"):
        self.data_root = Path(data_root)
        self.output_root = Path(output_root)
        self.output_root.mkdir(parents=True, exist_ok=True)
        
        # ç­›é€‰æ ‡å‡†
        self.criteria = {
            'multi_peak_overlap': {
                'description': 'å¤šå³°é‡å æ•°æ® - å…‰è°±ç»´åº¦å¤šä¸ªé‡å å³°',
                'min_peaks': 3,
                'overlap_threshold': 0.6,
                'peak_prominence_factor': 0.15
            },
            'transient_decay': {
                'description': 'ç¬æ€ä¿¡å·è¡°å‡æ•°æ® - æ—¶é—´ç»´åº¦æ˜æ˜¾è¡°å‡',
                'min_decay_ratio': 2.0,  # åˆå§‹/æœ€ç»ˆä¿¡å·æ¯”å€¼
                'exponential_fit_r2': 0.7,
                'time_constant_range': (0.1, 100)  # ps
            },
            'low_snr': {
                'description': 'ä½ä¿¡å™ªæ¯”æ•°æ® - SNR â‰¤ 5:1',
                'snr_threshold': 5.0,
                'noise_estimation_regions': 4  # è¾¹ç¼˜åŒºåŸŸæ•°é‡
            }
        }
        
        self.results = {
            'multi_peak_overlap': [],
            'transient_decay': [],
            'low_snr': [],
            'failed_files': []
        }
    
    def find_all_data_files(self):
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        data_files = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        patterns = ['*.csv', '*.txt', '*.dat']
        
        for pattern in patterns:
            files = list(self.data_root.rglob(pattern))
            data_files.extend(files)
        
        # è¿‡æ»¤æ‰æ˜æ˜¾çš„éæ•°æ®æ–‡ä»¶
        filtered_files = []
        for file in data_files:
            # è·³è¿‡å¤ªå°çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯é…ç½®æ–‡ä»¶ï¼‰
            if file.stat().st_size < 1000:  # å°äº1KB
                continue
            # è·³è¿‡æ˜æ˜¾çš„éæ•°æ®æ–‡ä»¶å
            if any(skip in file.name.lower() for skip in ['readme', 'config', 'log']):
                continue
            filtered_files.append(file)
        
        print(f"æ‰¾åˆ° {len(filtered_files)} ä¸ªæ½œåœ¨æ•°æ®æ–‡ä»¶")
        return filtered_files
    
    def load_tas_data(self, file_path):
        """åŠ è½½TASæ•°æ®æ–‡ä»¶"""
        try:
            # å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
            for sep in [',', '\t', ' ']:
                try:
                    # å°è¯•ç¬¬ä¸€è¡Œä¸ºheader
                    df = pd.read_csv(file_path, sep=sep, index_col=0)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼æ•°æ®
                    if df.select_dtypes(include=[np.number]).shape[1] < 3:
                        continue
                    
                    # åŸºæœ¬éªŒè¯
                    if df.shape[0] < 10 or df.shape[1] < 10:
                        continue
                    
                    # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                    df = df.select_dtypes(include=[np.number])
                    
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åƒæ³¢é•¿ï¼ˆé€šå¸¸300-800nmï¼‰
                    if df.index.dtype in [np.float64, np.int64]:
                        wavelengths = df.index.values
                        if 200 <= wavelengths.min() <= 1000 and 300 <= wavelengths.max() <= 1200:
                            # æ£€æŸ¥åˆ—æ˜¯å¦åƒæ—¶é—´å»¶è¿Ÿ
                            try:
                                time_delays = df.columns.astype(float)
                                if len(time_delays) > 5:
                                    return {
                                        'data': df.values,
                                        'wavelengths': wavelengths,
                                        'time_delays': time_delays.values,
                                        'shape': df.shape,
                                        'file_path': str(file_path)
                                    }
                            except:
                                continue
                    
                except Exception:
                    continue
            
            return None
            
        except Exception as e:
            print(f"åŠ è½½å¤±è´¥ {file_path.name}: {e}")
            return None
    
    def analyze_multi_peak_overlap(self, data_info):
        """åˆ†æå¤šå³°é‡å ç‰¹å¾"""
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        
        overlap_scores = []
        
        # åˆ†æå¤šä¸ªæ—¶é—´ç‚¹çš„å…‰è°±
        time_points_to_check = min(10, data.shape[1])
        step = max(1, data.shape[1] // time_points_to_check)
        
        for i in range(0, data.shape[1], step):
            spectrum = data[:, i]
            
            # å¹³æ»‘å…‰è°±ä»¥å‡å°‘å™ªå£°å½±å“
            if len(spectrum) > 20:
                from scipy.ndimage import gaussian_filter1d
                smoothed = gaussian_filter1d(spectrum, sigma=2)
            else:
                smoothed = spectrum
            
            # æ‰¾å³°ï¼ˆæ­£å³°å’Œè´Ÿå³°ï¼‰
            abs_spectrum = np.abs(smoothed)
            
            # åŠ¨æ€è°ƒæ•´å³°è¯†åˆ«å‚æ•°
            prominence = np.std(abs_spectrum) * self.criteria['multi_peak_overlap']['peak_prominence_factor']
            min_distance = len(abs_spectrum) // 30
            
            peaks, properties = signal.find_peaks(
                abs_spectrum,
                prominence=prominence,
                distance=min_distance
            )
            
            if len(peaks) >= self.criteria['multi_peak_overlap']['min_peaks']:
                # è®¡ç®—å³°å®½å’Œé—´è·
                try:
                    peak_widths = signal.peak_widths(abs_spectrum, peaks, rel_height=0.5)[0]
                    
                    if len(peaks) > 1:
                        peak_distances = np.diff(peaks)
                        avg_width = np.mean(peak_widths)
                        avg_distance = np.mean(peak_distances)
                        
                        # é‡å åº¦ = å¹³å‡å³°å®½ / å¹³å‡å³°é—´è·
                        overlap_ratio = avg_width / avg_distance if avg_distance > 0 else 0
                        
                        if overlap_ratio > self.criteria['multi_peak_overlap']['overlap_threshold']:
                            overlap_scores.append({
                                'time_index': i,
                                'num_peaks': len(peaks),
                                'overlap_ratio': overlap_ratio,
                                'peak_positions': peaks,
                                'peak_intensities': abs_spectrum[peaks]
                            })
                
                except Exception:
                    continue
        
        if overlap_scores:
            avg_overlap = np.mean([s['overlap_ratio'] for s in overlap_scores])
            max_peaks = max([s['num_peaks'] for s in overlap_scores])
            
            return {
                'is_multi_peak': True,
                'score': avg_overlap,
                'max_peaks': max_peaks,
                'details': overlap_scores[:3]  # ä¿å­˜å‰3ä¸ªè¯¦ç»†ä¿¡æ¯
            }
        
        return {'is_multi_peak': False, 'score': 0}
    
    def analyze_transient_decay(self, data_info):
        """åˆ†æç¬æ€ä¿¡å·è¡°å‡ç‰¹å¾"""
        data = data_info['data']
        time_delays = data_info['time_delays']
        
        # åªåˆ†ææ­£æ—¶é—´å»¶è¿Ÿ
        positive_time_mask = time_delays > 0
        if np.sum(positive_time_mask) < 5:
            return {'is_transient': False, 'score': 0}
        
        pos_times = time_delays[positive_time_mask]
        pos_data = data[:, positive_time_mask]
        
        decay_results = []
        
        # åˆ†æå¤šä¸ªæ³¢é•¿çš„è¡°å‡è¡Œä¸º
        wavelength_step = max(1, pos_data.shape[0] // 20)
        
        for i in range(0, pos_data.shape[0], wavelength_step):
            kinetic = pos_data[i, :]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¡°å‡è¶‹åŠ¿
            if len(kinetic) < 5:
                continue
            
            initial_signal = np.mean(kinetic[:3])  # å‰3ä¸ªç‚¹
            final_signal = np.mean(kinetic[-3:])   # å3ä¸ªç‚¹
            
            if abs(initial_signal) < 1e-10:  # é¿å…é™¤é›¶
                continue
            
            decay_ratio = abs(initial_signal / final_signal) if abs(final_signal) > 1e-10 else float('inf')
            
            if decay_ratio >= self.criteria['transient_decay']['min_decay_ratio']:
                try:
                    # æŒ‡æ•°è¡°å‡æ‹Ÿåˆ
                    def exp_decay(t, a, tau, c):
                        return a * np.exp(-t / tau) + c
                    
                    # åˆå§‹çŒœæµ‹
                    a_guess = initial_signal - final_signal
                    tau_guess = pos_times[len(pos_times)//2]
                    c_guess = final_signal
                    
                    popt, pcov = curve_fit(
                        exp_decay,
                        pos_times,
                        kinetic,
                        p0=[a_guess, tau_guess, c_guess],
                        bounds=([-np.inf, 0.01, -np.inf], [np.inf, 1000, np.inf]),
                        maxfev=1000
                    )
                    
                    # è®¡ç®—æ‹Ÿåˆè´¨é‡
                    fitted = exp_decay(pos_times, *popt)
                    ss_res = np.sum((kinetic - fitted) ** 2)
                    ss_tot = np.sum((kinetic - np.mean(kinetic)) ** 2)
                    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
                    
                    tau = abs(popt[1])
                    
                    if (r2 >= self.criteria['transient_decay']['exponential_fit_r2'] and
                        self.criteria['transient_decay']['time_constant_range'][0] <= tau <= 
                        self.criteria['transient_decay']['time_constant_range'][1]):
                        
                        decay_results.append({
                            'wavelength_index': i,
                            'time_constant': tau,
                            'r2': r2,
                            'decay_ratio': decay_ratio,
                            'amplitude': popt[0]
                        })
                
                except Exception:
                    continue
        
        if decay_results:
            avg_r2 = np.mean([r['r2'] for r in decay_results])
            avg_tau = np.mean([r['time_constant'] for r in decay_results])
            
            return {
                'is_transient': True,
                'score': avg_r2,
                'avg_time_constant': avg_tau,
                'num_decay_components': len(decay_results),
                'details': decay_results[:3]
            }
        
        return {'is_transient': False, 'score': 0}
    
    def analyze_low_snr(self, data_info):
        """åˆ†æä¿¡å™ªæ¯”"""
        data = data_info['data']
        
        # ä¼°è®¡å™ªå£°æ°´å¹³ï¼ˆä½¿ç”¨æ•°æ®è¾¹ç¼˜ï¼‰
        h, w = data.shape
        edge_size = min(5, h//10, w//10)
        
        if edge_size == 0:
            return {'is_low_snr': False, 'snr': float('inf')}
        
        # ä»å››ä¸ªè§’è½ä¼°è®¡å™ªå£°
        noise_regions = [
            data[:edge_size, :edge_size],      # å·¦ä¸Š
            data[:edge_size, -edge_size:],     # å³ä¸Š
            data[-edge_size:, :edge_size],     # å·¦ä¸‹
            data[-edge_size:, -edge_size:]     # å³ä¸‹
        ]
        
        noise_stds = [np.std(region) for region in noise_regions if region.size > 0]
        
        if not noise_stds:
            return {'is_low_snr': False, 'snr': float('inf')}
        
        noise_level = np.mean(noise_stds)
        
        # ä¼°è®¡ä¿¡å·å¼ºåº¦ï¼ˆä¸­å¿ƒåŒºåŸŸçš„æœ€å¤§ç»å¯¹å€¼ï¼‰
        center_h_start = h//4
        center_h_end = 3*h//4
        center_w_start = w//4
        center_w_end = 3*w//4
        
        center_data = data[center_h_start:center_h_end, center_w_start:center_w_end]
        signal_strength = np.max(np.abs(center_data)) if center_data.size > 0 else 0
        
        # è®¡ç®—SNR
        snr = signal_strength / noise_level if noise_level > 0 else float('inf')
        
        return {
            'is_low_snr': snr <= self.criteria['low_snr']['snr_threshold'],
            'snr': snr,
            'noise_level': noise_level,
            'signal_strength': signal_strength
        }
    
    def create_visualization(self, data_info, analysis_results, category, output_path):
        """åˆ›å»ºæ•°æ®å¯è§†åŒ–å›¾"""
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        time_delays = data_info['time_delays']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{category} - {Path(data_info["file_path"]).name}', fontsize=16, fontweight='bold')
        
        # 1. çƒ­å›¾
        ax1 = axes[0, 0]
        im1 = ax1.imshow(data, aspect='auto', cmap='RdBu_r', 
                        extent=[time_delays.min(), time_delays.max(), 
                               wavelengths.max(), wavelengths.min()])
        ax1.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax1.set_ylabel('æ³¢é•¿ (nm)')
        ax1.set_title('2Då…‰è°±çƒ­å›¾')
        plt.colorbar(im1, ax=ax1, label='Î”A')
        
        # 2. é€‰æ‹©æ€§æ—¶é—´åˆ‡ç‰‡å…‰è°±
        ax2 = axes[0, 1]
        time_indices = [0, len(time_delays)//4, len(time_delays)//2, -1]
        colors = ['red', 'blue', 'green', 'orange']
        
        for i, (idx, color) in enumerate(zip(time_indices, colors)):
            if idx < len(time_delays):
                spectrum = data[:, idx]
                label = f't = {time_delays[idx]:.2f} ps'
                ax2.plot(wavelengths, spectrum, color=color, label=label, linewidth=2)
        
        ax2.set_xlabel('æ³¢é•¿ (nm)')
        ax2.set_ylabel('Î”A')
        ax2.set_title('ä¸åŒæ—¶é—´çš„å…‰è°±åˆ‡ç‰‡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. é€‰æ‹©æ€§æ³¢é•¿åŠ¨åŠ›å­¦
        ax3 = axes[1, 0]
        wl_indices = [len(wavelengths)//4, len(wavelengths)//2, 3*len(wavelengths)//4]
        
        for i, idx in enumerate(wl_indices):
            if idx < len(wavelengths):
                kinetic = data[idx, :]
                label = f'Î» = {wavelengths[idx]:.0f} nm'
                ax3.plot(time_delays, kinetic, label=label, linewidth=2, marker='o', markersize=3)
        
        ax3.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax3.set_ylabel('Î”A')
        ax3.set_title('ä¸åŒæ³¢é•¿çš„åŠ¨åŠ›å­¦æ›²çº¿')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç‰¹å¾åˆ†æå›¾
        ax4 = axes[1, 1]
        
        if category == 'multi_peak_overlap':
            # æ˜¾ç¤ºå³°è¯†åˆ«ç»“æœ
            if analysis_results['is_multi_peak']:
                detail = analysis_results['details'][0]
                spectrum = data[:, detail['time_index']]
                ax4.plot(wavelengths, np.abs(spectrum), 'b-', linewidth=2, label='å…‰è°±')
                
                # æ ‡è®°è¯†åˆ«çš„å³°
                peak_positions = detail['peak_positions']
                peak_wavelengths = wavelengths[peak_positions]
                peak_intensities = detail['peak_intensities']
                
                ax4.plot(peak_wavelengths, peak_intensities, 'ro', markersize=8, label=f'å³° (n={len(peak_positions)})')
                ax4.set_title(f'å¤šå³°è¯†åˆ«\né‡å åº¦: {detail["overlap_ratio"]:.3f}')
            else:
                ax4.plot(wavelengths, np.abs(data[:, 0]), 'b-', linewidth=2)
                ax4.set_title('æœªæ£€æµ‹åˆ°å¤šå³°é‡å ')
        
        elif category == 'transient_decay':
            # æ˜¾ç¤ºè¡°å‡æ‹Ÿåˆ
            if analysis_results['is_transient']:
                detail = analysis_results['details'][0]
                wl_idx = detail['wavelength_index']
                kinetic = data[wl_idx, time_delays > 0]
                pos_times = time_delays[time_delays > 0]
                
                ax4.plot(pos_times, kinetic, 'bo', markersize=4, label='å®éªŒæ•°æ®')
                
                # æ‹Ÿåˆæ›²çº¿
                def exp_decay(t, a, tau, c):
                    return a * np.exp(-t / tau) + c
                
                tau = detail['time_constant']
                # ç®€å•ä¼°è®¡å‚æ•°ç”¨äºæ˜¾ç¤º
                a_est = kinetic[0] - kinetic[-1]
                c_est = kinetic[-1]
                fitted = exp_decay(pos_times, a_est, tau, c_est)
                
                ax4.plot(pos_times, fitted, 'r-', linewidth=2, 
                        label=f'æ‹Ÿåˆ (Ï„={tau:.2f}ps, RÂ²={detail["r2"]:.3f})')
                
                ax4.set_title(f'ç¬æ€è¡°å‡æ‹Ÿåˆ\nÎ» = {wavelengths[wl_idx]:.0f} nm')
                ax4.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
            else:
                ax4.plot(time_delays, data[len(wavelengths)//2, :], 'b-', linewidth=2)
                ax4.set_title('æœªæ£€æµ‹åˆ°æ˜æ˜¾è¡°å‡')
        
        elif category == 'low_snr':
            # æ˜¾ç¤ºä¿¡å™ªæ¯”åˆ†æ
            snr = analysis_results['snr']
            
            # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒç›´æ–¹å›¾
            ax4.hist(data.flatten(), bins=50, alpha=0.7, density=True, color='skyblue')
            ax4.axvline(0, color='red', linestyle='--', alpha=0.7, label='é›¶åŸºçº¿')
            ax4.set_title(f'æ•°æ®åˆ†å¸ƒ\nSNR = {snr:.2f}')
            ax4.set_xlabel('ä¿¡å·å¼ºåº¦')
            ax4.set_ylabel('å¯†åº¦')
        
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def screen_all_files(self):
        """ç­›é€‰æ‰€æœ‰æ–‡ä»¶"""
        print("å¼€å§‹ç­›é€‰TASæ•°æ®æ–‡ä»¶...")
        print("=" * 60)
        
        # æ‰¾åˆ°æ‰€æœ‰æ•°æ®æ–‡ä»¶
        data_files = self.find_all_data_files()
        
        if not data_files:
            print("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼")
            return
        
        total_files = len(data_files)
        processed = 0
        
        for file_path in data_files:
            processed += 1
            print(f"\n[{processed}/{total_files}] åˆ†æ: {file_path.relative_to(self.data_root)}")
            
            # åŠ è½½æ•°æ®
            data_info = self.load_tas_data(file_path)
            if data_info is None:
                self.results['failed_files'].append(str(file_path))
                print(f"  âŒ åŠ è½½å¤±è´¥")
                continue
            
            print(f"  ğŸ“Š æ•°æ®å½¢çŠ¶: {data_info['shape']} (æ³¢é•¿Ã—æ—¶é—´)")
            
            # åˆ†æä¸‰ç±»ç‰¹å¾
            multi_peak = self.analyze_multi_peak_overlap(data_info)
            transient = self.analyze_transient_decay(data_info)
            low_snr = self.analyze_low_snr(data_info)
            
            # è®°å½•ç»“æœ
            file_result = {
                'file_path': str(file_path),
                'relative_path': str(file_path.relative_to(self.data_root)),
                'shape': data_info['shape'],
                'multi_peak': multi_peak,
                'transient': transient,
                'low_snr': low_snr
            }
            
            # åˆ†ç±»å¹¶åˆ›å»ºå¯è§†åŒ–
            categories_found = []
            
            if multi_peak['is_multi_peak']:
                self.results['multi_peak_overlap'].append(file_result)
                categories_found.append('multi_peak_overlap')
                print(f"  âœ… å¤šå³°é‡å : è¯„åˆ†={multi_peak['score']:.3f}, å³°æ•°={multi_peak['max_peaks']}")
            
            if transient['is_transient']:
                self.results['transient_decay'].append(file_result)
                categories_found.append('transient_decay')
                print(f"  âœ… ç¬æ€è¡°å‡: RÂ²={transient['score']:.3f}, Ï„å¹³å‡={transient['avg_time_constant']:.2f}ps")
            
            if low_snr['is_low_snr']:
                self.results['low_snr'].append(file_result)
                categories_found.append('low_snr')
                print(f"  âœ… ä½ä¿¡å™ªæ¯”: SNR={low_snr['snr']:.2f}")
            
            # ä¸ºæ¯ä¸ªç¬¦åˆæ¡ä»¶çš„ç±»åˆ«åˆ›å»ºå¯è§†åŒ–
            for category in categories_found:
                self.create_category_visualization(data_info, file_result, category)
            
            if not categories_found:
                print(f"  â– å¸¸è§„æ•°æ® (SNR={low_snr['snr']:.2f})")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_summary_report()
        self.save_results()
    
    def create_category_visualization(self, data_info, file_result, category):
        """ä¸ºç‰¹å®šç±»åˆ«åˆ›å»ºå¯è§†åŒ–"""
        # åˆ›å»ºç±»åˆ«è¾“å‡ºç›®å½•
        category_dir = self.output_root / category
        category_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_name = Path(file_result['relative_path']).stem.replace(' ', '_').replace('/', '_')
        output_file = category_dir / f"{safe_name}.png"
        
        # åˆ›å»ºå¯è§†åŒ–
        analysis_result = file_result[category.split('_')[0] if category != 'multi_peak_overlap' else 'multi_peak']
        
        try:
            self.create_visualization(data_info, analysis_result, 
                                    self.get_category_chinese_name(category), 
                                    output_file)
            print(f"    ğŸ’¾ å¯è§†åŒ–ä¿å­˜: {output_file.relative_to(self.output_root)}")
        except Exception as e:
            print(f"    âŒ å¯è§†åŒ–å¤±è´¥: {e}")
    
    def get_category_chinese_name(self, category):
        """è·å–ç±»åˆ«ä¸­æ–‡åç§°"""
        names = {
            'multi_peak_overlap': 'å¤šå³°é‡å æ•°æ®',
            'transient_decay': 'ç¬æ€ä¿¡å·è¡°å‡æ•°æ®',
            'low_snr': 'ä½ä¿¡å™ªæ¯”æ•°æ®'
        }
        return names.get(category, category)
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report_file = self.output_root / "TASæ•°æ®ç­›é€‰æŠ¥å‘Š.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# TASæ•°æ®ç­›é€‰æŠ¥å‘Š\n\n")
            f.write(f"ç­›é€‰æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®æºç›®å½•: {self.data_root}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            f.write("## ğŸ“Š ç­›é€‰ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»æ–‡ä»¶æ•°**: {sum(len(v) if isinstance(v, list) else 0 for v in self.results.values())}\n")
            f.write(f"- **å¤šå³°é‡å æ•°æ®**: {len(self.results['multi_peak_overlap'])} ä¸ª\n")
            f.write(f"- **ç¬æ€è¡°å‡æ•°æ®**: {len(self.results['transient_decay'])} ä¸ª\n")
            f.write(f"- **ä½ä¿¡å™ªæ¯”æ•°æ®**: {len(self.results['low_snr'])} ä¸ª\n")
            f.write(f"- **åŠ è½½å¤±è´¥**: {len(self.results['failed_files'])} ä¸ª\n\n")
            
            # è¯¦ç»†åˆ—è¡¨
            for category, chinese_name in [
                ('multi_peak_overlap', 'å¤šå³°é‡å æ•°æ®'),
                ('transient_decay', 'ç¬æ€ä¿¡å·è¡°å‡æ•°æ®'),
                ('low_snr', 'ä½ä¿¡å™ªæ¯”æ•°æ®')
            ]:
                f.write(f"## ğŸ¯ {chinese_name}\n\n")
                
                if self.results[category]:
                    f.write("| åºå· | æ–‡ä»¶è·¯å¾„ | æ•°æ®å½¢çŠ¶ | ç‰¹å¾è¯„åˆ† | å¯è§†åŒ– |\n")
                    f.write("|------|----------|----------|----------|--------|\n")
                    
                    for i, item in enumerate(self.results[category], 1):
                        rel_path = item['relative_path']
                        shape = f"{item['shape'][0]}Ã—{item['shape'][1]}"
                        
                        # è·å–ç‰¹å¾è¯„åˆ†
                        if category == 'multi_peak_overlap':
                            score = f"é‡å åº¦={item['multi_peak']['score']:.3f}"
                        elif category == 'transient_decay':
                            score = f"RÂ²={item['transient']['score']:.3f}"
                        else:  # low_snr
                            score = f"SNR={item['low_snr']['snr']:.2f}"
                        
                        # å¯è§†åŒ–æ–‡ä»¶
                        safe_name = Path(rel_path).stem.replace(' ', '_').replace('/', '_')
                        viz_file = f"{category}/{safe_name}.png"
                        
                        f.write(f"| {i} | `{rel_path}` | {shape} | {score} | ![viz]({viz_file}) |\n")
                    
                    f.write("\n")
                else:
                    f.write("æš‚æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®æ–‡ä»¶ã€‚\n\n")
            
            # ç­›é€‰æ ‡å‡†
            f.write("## âš™ï¸ ç­›é€‰æ ‡å‡†\n\n")
            for category, criteria in self.criteria.items():
                chinese_name = self.get_category_chinese_name(category)
                f.write(f"### {chinese_name}\n")
                f.write(f"- **æè¿°**: {criteria['description']}\n")
                
                if category == 'multi_peak_overlap':
                    f.write(f"- **æœ€å°‘å³°æ•°**: {criteria['min_peaks']}\n")
                    f.write(f"- **é‡å é˜ˆå€¼**: {criteria['overlap_threshold']}\n")
                elif category == 'transient_decay':
                    f.write(f"- **æœ€å°è¡°å‡æ¯”**: {criteria['min_decay_ratio']}\n")
                    f.write(f"- **æ‹ŸåˆRÂ²é˜ˆå€¼**: {criteria['exponential_fit_r2']}\n")
                    f.write(f"- **æ—¶é—´å¸¸æ•°èŒƒå›´**: {criteria['time_constant_range']} ps\n")
                elif category == 'low_snr':
                    f.write(f"- **SNRé˜ˆå€¼**: â‰¤ {criteria['snr_threshold']}\n")
                
                f.write("\n")
            
            # ä½¿ç”¨å»ºè®®
            f.write("## ğŸ’¡ ä½¿ç”¨å»ºè®®\n\n")
            f.write("### å¤šå³°é‡å æ•°æ®\n")
            f.write("- é€‚åˆæµ‹è¯•MCR-ALSåœ¨å¤æ‚å…‰è°±é‡å æƒ…å†µä¸‹çš„åˆ†è¾¨èƒ½åŠ›\n")
            f.write("- å»ºè®®ä½¿ç”¨æ›´ä¸¥æ ¼çš„çº¦æŸæ¡ä»¶å’Œæ›´å¤šç»„åˆ†\n\n")
            
            f.write("### ç¬æ€ä¿¡å·è¡°å‡æ•°æ®\n")
            f.write("- é€‚åˆéªŒè¯æ—¶é—´åˆ†è¾¨MCR-ALSçš„åŠ¨åŠ›å­¦åˆ†æèƒ½åŠ›\n")
            f.write("- å…³æ³¨è¡°å‡æ—¶é—´å¸¸æ•°çš„ç‰©ç†åˆç†æ€§\n\n")
            
            f.write("### ä½ä¿¡å™ªæ¯”æ•°æ®\n")
            f.write("- é€‚åˆæµ‹è¯•ç®—æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§\n")
            f.write("- å»ºè®®å¢åŠ éšæœºåˆå§‹åŒ–æ¬¡æ•°å’Œé¢„å¤„ç†æ­¥éª¤\n\n")
        
        print(f"\nğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def save_results(self):
        """ä¿å­˜ç­›é€‰ç»“æœ"""
        # å¤„ç†numpyç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            return obj
        
        results_clean = convert_numpy(self.results)
        
        output_file = self.output_root / "screening_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_clean, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” TASæŒ‘æˆ˜æ€§æ•°æ®ç­›é€‰å™¨")
    print("=" * 50)
    
    screener = TASDataScreener()
    screener.screen_all_files()
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ‰ ç­›é€‰å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {screener.output_root}")
    print(f"ğŸ¯ å¤šå³°é‡å æ•°æ®: {len(screener.results['multi_peak_overlap'])} ä¸ª")
    print(f"âš¡ ç¬æ€è¡°å‡æ•°æ®: {len(screener.results['transient_decay'])} ä¸ª")
    print(f"ğŸ“‰ ä½ä¿¡å™ªæ¯”æ•°æ®: {len(screener.results['low_snr'])} ä¸ª")
    
    # æ˜¾ç¤ºè¾“å‡ºç›®å½•ç»“æ„
    print(f"\nğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
    for item in screener.output_root.iterdir():
        if item.is_dir():
            file_count = len(list(item.glob('*.png')))
            print(f"  ğŸ“ {item.name}/ ({file_count} ä¸ªå¯è§†åŒ–æ–‡ä»¶)")
        else:
            print(f"  ğŸ“„ {item.name}")

if __name__ == "__main__":
    main()