#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TASæ•°æ®ç­›é€‰å™¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œé’ˆå¯¹å·²çŸ¥çš„å·¥ä½œæ•°æ®æ–‡ä»¶
ä¸“é—¨ç­›é€‰ä¸‰ç±»æŒ‘æˆ˜æ€§æ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from scipy import signal
from scipy.optimize import curve_fit
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SimpleTASScreener:
    """ç®€åŒ–çš„TASæ•°æ®ç­›é€‰å™¨"""
    
    def __init__(self, output_root="experiments/results/data_screening"):
        self.output_root = Path(output_root)
        self.output_root.mkdir(parents=True, exist_ok=True)
        
        # å·²çŸ¥çš„å·¥ä½œæ•°æ®æ–‡ä»¶
        self.known_files = [
            "data/TAS/TA_Average.csv",
            "data/TAS/TA_Average-crop-TCA-tzs-tzs-tzs-chirp.csv"
        ]
        
        self.results = {
            'multi_peak_overlap': [],
            'transient_decay': [],
            'low_snr': [],
            'analysis_summary': {}
        }
    
    def load_tas_file(self, file_path):
        """åŠ è½½TASæ–‡ä»¶"""
        try:
            # è¯»å–CSVæ–‡ä»¶ï¼Œç¬¬ä¸€è¡Œä¸ºæ—¶é—´å»¶è¿Ÿï¼Œç¬¬ä¸€åˆ—ä¸ºæ³¢é•¿
            df = pd.read_csv(file_path, index_col=0)
            
            # è·å–æ³¢é•¿å’Œæ—¶é—´å»¶è¿Ÿ
            wavelengths = df.index.values.astype(float)
            time_delays = df.columns.astype(float)
            data = df.values
            
            # åŸºæœ¬éªŒè¯
            if data.shape[0] < 10 or data.shape[1] < 10:
                return None
            
            # å¤„ç†å¼‚å¸¸å€¼
            data = np.where(np.isfinite(data), data, 0)
            
            return {
                'data': data,
                'wavelengths': wavelengths,
                'time_delays': time_delays,
                'shape': data.shape,
                'file_path': str(file_path)
            }
            
        except Exception as e:
            print(f"åŠ è½½å¤±è´¥ {file_path}: {e}")
            return None
    
    def analyze_multi_peak_overlap(self, data_info):
        """åˆ†æå¤šå³°é‡å ç‰¹å¾"""
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        
        # å¯»æ‰¾æœ‰æ˜æ˜¾ä¿¡å·çš„æ—¶é—´ç‚¹
        signal_strength = np.max(np.abs(data), axis=0)
        strong_signal_idx = np.where(signal_strength > np.percentile(signal_strength, 70))[0]
        
        overlap_scores = []
        
        for idx in strong_signal_idx[:10]:  # æ£€æŸ¥å‰10ä¸ªå¼ºä¿¡å·æ—¶é—´ç‚¹
            spectrum = data[:, idx]
            abs_spectrum = np.abs(spectrum)
            
            # æ‰¾å³°
            prominence = np.std(abs_spectrum) * 0.2
            peaks, properties = signal.find_peaks(
                abs_spectrum,
                prominence=prominence,
                distance=len(abs_spectrum) // 20
            )
            
            if len(peaks) >= 3:  # è‡³å°‘3ä¸ªå³°
                # è®¡ç®—å³°é—´é‡å 
                try:
                    peak_widths = signal.peak_widths(abs_spectrum, peaks, rel_height=0.5)[0]
                    peak_distances = np.diff(peaks)
                    
                    if len(peak_distances) > 0:
                        overlap_ratio = np.mean(peak_widths[:-1]) / np.mean(peak_distances)
                        
                        if overlap_ratio > 0.5:  # é‡å é˜ˆå€¼
                            overlap_scores.append({
                                'time_index': idx,
                                'time_delay': data_info['time_delays'][idx],
                                'num_peaks': len(peaks),
                                'overlap_ratio': overlap_ratio,
                                'peaks': peaks,
                                'peak_heights': abs_spectrum[peaks]
                            })
                except:
                    continue
        
        if overlap_scores:
            avg_overlap = np.mean([s['overlap_ratio'] for s in overlap_scores])
            max_peaks = max([s['num_peaks'] for s in overlap_scores])
            
            return {
                'is_multi_peak': True,
                'score': avg_overlap,
                'max_peaks': max_peaks,
                'num_overlap_times': len(overlap_scores),
                'details': overlap_scores
            }
        
        return {'is_multi_peak': False, 'score': 0}
    
    def analyze_transient_decay(self, data_info):
        """åˆ†æç¬æ€è¡°å‡ç‰¹å¾"""
        data = data_info['data']
        time_delays = data_info['time_delays']
        
        # åªåˆ†ææ­£æ—¶é—´å»¶è¿Ÿ
        pos_mask = time_delays > 0
        if np.sum(pos_mask) < 5:
            return {'is_transient': False, 'score': 0}
        
        pos_times = time_delays[pos_mask]
        pos_data = data[:, pos_mask]
        
        decay_results = []
        
        # åˆ†ææœ‰å¼ºä¿¡å·çš„æ³¢é•¿
        max_signal_per_wavelength = np.max(np.abs(pos_data), axis=1)
        strong_wavelengths = np.where(max_signal_per_wavelength > np.percentile(max_signal_per_wavelength, 80))[0]
        
        for wl_idx in strong_wavelengths[:15]:  # æ£€æŸ¥å‰15ä¸ªå¼ºä¿¡å·æ³¢é•¿
            kinetic = pos_data[wl_idx, :]
            
            # æ£€æŸ¥è¡°å‡è¶‹åŠ¿
            if len(kinetic) < 5:
                continue
                
            initial = np.mean(kinetic[:3])
            final = np.mean(kinetic[-3:])
            
            if abs(initial) < 1e-10:
                continue
                
            decay_ratio = abs(initial / final) if abs(final) > 1e-10 else 100
            
            if decay_ratio > 1.5:  # æœ‰è¡°å‡è¶‹åŠ¿
                try:
                    # ç®€å•çš„æŒ‡æ•°è¡°å‡æ‹Ÿåˆ
                    def exp_decay(t, a, tau, c):
                        return a * np.exp(-t / tau) + c
                    
                    # åˆå§‹çŒœæµ‹
                    popt, _ = curve_fit(
                        exp_decay,
                        pos_times,
                        kinetic,
                        p0=[initial - final, pos_times[len(pos_times)//2], final],
                        bounds=([-np.inf, 0.01, -np.inf], [np.inf, 1000, np.inf]),
                        maxfev=500
                    )
                    
                    # è®¡ç®—æ‹Ÿåˆè´¨é‡
                    fitted = exp_decay(pos_times, *popt)
                    r2 = 1 - np.sum((kinetic - fitted)**2) / np.sum((kinetic - np.mean(kinetic))**2)
                    
                    if r2 > 0.6 and 0.1 <= abs(popt[1]) <= 100:  # æ—¶é—´å¸¸æ•°åˆç†
                        decay_results.append({
                            'wavelength_index': wl_idx,
                            'wavelength': data_info['wavelengths'][wl_idx],
                            'time_constant': abs(popt[1]),
                            'r2': r2,
                            'decay_ratio': decay_ratio,
                            'amplitude': popt[0]
                        })
                
                except:
                    continue
        
        if decay_results:
            avg_r2 = np.mean([r['r2'] for r in decay_results])
            avg_tau = np.mean([r['time_constant'] for r in decay_results])
            
            return {
                'is_transient': True,
                'score': avg_r2,
                'avg_time_constant': avg_tau,
                'num_decay_wavelengths': len(decay_results),
                'details': decay_results
            }
        
        return {'is_transient': False, 'score': 0}
    
    def analyze_low_snr(self, data_info):
        """åˆ†æä¿¡å™ªæ¯”"""
        data = data_info['data']
        
        # ä¼°è®¡å™ªå£°ï¼ˆä½¿ç”¨è¾¹ç¼˜åŒºåŸŸï¼‰
        h, w = data.shape
        edge_size = max(2, min(h//15, w//15))
        
        noise_regions = [
            data[:edge_size, :edge_size],
            data[:edge_size, -edge_size:],
            data[-edge_size:, :edge_size],
            data[-edge_size:, -edge_size:]
        ]
        
        noise_stds = [np.std(region) for region in noise_regions if region.size > 0]
        noise_level = np.mean(noise_stds) if noise_stds else 0
        
        # ä¼°è®¡ä¿¡å·å¼ºåº¦ï¼ˆä¸­å¿ƒåŒºåŸŸæœ€å¤§å€¼ï¼‰
        center_h = slice(h//4, 3*h//4)
        center_w = slice(w//4, 3*w//4)
        center_data = data[center_h, center_w]
        signal_strength = np.max(np.abs(center_data))
        
        # è®¡ç®—SNR
        snr = signal_strength / noise_level if noise_level > 0 else float('inf')
        
        return {
            'is_low_snr': snr <= 5.0,
            'snr': snr,
            'noise_level': noise_level,
            'signal_strength': signal_strength
        }
    
    def create_comprehensive_visualization(self, data_info, analyses, output_file):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        time_delays = data_info['time_delays']
        file_name = Path(data_info['file_path']).name
        
        fig = plt.figure(figsize=(20, 15))
        
        # 1. ä¸»è¦2Dçƒ­å›¾
        ax1 = plt.subplot(3, 4, (1, 2))
        im = ax1.imshow(data, aspect='auto', cmap='RdBu_r',
                       extent=[time_delays.min(), time_delays.max(),
                              wavelengths.max(), wavelengths.min()])
        ax1.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax1.set_ylabel('æ³¢é•¿ (nm)')
        ax1.set_title(f'TAS 2D å…‰è°±çƒ­å›¾\n{file_name}', fontweight='bold')
        plt.colorbar(im, ax=ax1, label='Î”A')
        
        # 2. æ—¶é—´åˆ‡ç‰‡å…‰è°±ï¼ˆå¤šå³°é‡å åˆ†æï¼‰
        ax2 = plt.subplot(3, 4, 3)
        if analyses['multi_peak']['is_multi_peak']:
            detail = analyses['multi_peak']['details'][0]
            t_idx = detail['time_index']
            spectrum = data[:, t_idx]
            peaks = detail['peaks']
            
            ax2.plot(wavelengths, np.abs(spectrum), 'b-', linewidth=2, label='å…‰è°±')
            if len(peaks) > 0:
                ax2.plot(wavelengths[peaks], np.abs(spectrum[peaks]), 'ro', 
                        markersize=8, label=f'å³° (n={len(peaks)})')
            ax2.set_title(f'å¤šå³°è¯†åˆ«\nt={time_delays[t_idx]:.2f}ps\né‡å åº¦:{detail["overlap_ratio"]:.3f}')
        else:
            ax2.plot(wavelengths, np.abs(data[:, len(time_delays)//4]), 'b-', linewidth=2)
            ax2.set_title('æœªæ£€æµ‹åˆ°å¤šå³°é‡å ')
        
        ax2.set_xlabel('æ³¢é•¿ (nm)')
        ax2.set_ylabel('|Î”A|')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. åŠ¨åŠ›å­¦æ›²çº¿ï¼ˆç¬æ€è¡°å‡åˆ†æï¼‰
        ax3 = plt.subplot(3, 4, 4)
        if analyses['transient']['is_transient']:
            detail = analyses['transient']['details'][0]
            wl_idx = detail['wavelength_index']
            kinetic = data[wl_idx, time_delays > 0]
            pos_times = time_delays[time_delays > 0]
            
            ax3.semilogy(pos_times, np.abs(kinetic), 'bo', markersize=4, label='å®éªŒæ•°æ®')
            
            # æ‹Ÿåˆæ›²çº¿
            tau = detail['time_constant']
            amp = detail['amplitude']
            fitted = amp * np.exp(-pos_times / tau) + kinetic[-1]
            ax3.semilogy(pos_times, np.abs(fitted), 'r-', linewidth=2,
                        label=f'æ‹Ÿåˆ (Ï„={tau:.2f}ps)')
            
            ax3.set_title(f'ç¬æ€è¡°å‡\nÎ»={wavelengths[wl_idx]:.0f}nm\nRÂ²={detail["r2"]:.3f}')
        else:
            center_wl = len(wavelengths) // 2
            kinetic = data[center_wl, time_delays > 0]
            pos_times = time_delays[time_delays > 0]
            ax3.plot(pos_times, kinetic, 'b-', linewidth=2)
            ax3.set_title('æœªæ£€æµ‹åˆ°æ˜æ˜¾è¡°å‡')
        
        ax3.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax3.set_ylabel('Î”A')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ä¿¡å™ªæ¯”åˆ†æ
        ax4 = plt.subplot(3, 4, 5)
        snr = analyses['low_snr']['snr']
        data_flat = data.flatten()
        
        ax4.hist(data_flat, bins=50, alpha=0.7, density=True, color='skyblue')
        ax4.axvline(0, color='red', linestyle='--', alpha=0.7, label='é›¶åŸºçº¿')
        ax4.set_title(f'æ•°æ®åˆ†å¸ƒ\nSNR = {snr:.2f}')
        ax4.set_xlabel('ä¿¡å·å¼ºåº¦')
        ax4.set_ylabel('å¯†åº¦')
        ax4.legend()
        
        # 5-8. ä¸åŒæ—¶é—´ç‚¹çš„å…‰è°±
        time_indices = [0, len(time_delays)//4, len(time_delays)//2, 3*len(time_delays)//4]
        colors = ['red', 'blue', 'green', 'orange']
        
        for i, (t_idx, color) in enumerate(zip(time_indices, colors)):
            ax = plt.subplot(3, 4, 6+i)
            spectrum = data[:, t_idx]
            ax.plot(wavelengths, spectrum, color=color, linewidth=2)
            ax.set_title(f't = {time_delays[t_idx]:.2f} ps')
            ax.set_xlabel('æ³¢é•¿ (nm)')
            ax.set_ylabel('Î”A')
            ax.grid(True, alpha=0.3)
        
        # 9-12. ä¸åŒæ³¢é•¿çš„åŠ¨åŠ›å­¦
        wl_indices = [len(wavelengths)//6, len(wavelengths)//3, 2*len(wavelengths)//3, 5*len(wavelengths)//6]
        
        for i, wl_idx in enumerate(wl_indices):
            ax = plt.subplot(3, 4, 10+i)
            kinetic = data[wl_idx, :]
            ax.plot(time_delays, kinetic, linewidth=2, color=colors[i])
            ax.set_title(f'Î» = {wavelengths[wl_idx]:.0f} nm')
            ax.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
            ax.set_ylabel('Î”A')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
    
    def run_screening(self):
        """è¿è¡Œç­›é€‰"""
        print("ğŸ” å¼€å§‹ç­›é€‰TASæŒ‘æˆ˜æ€§æ•°æ®")
        print("=" * 50)
        
        for file_path in self.known_files:
            if not Path(file_path).exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {Path(file_path).name}")
            
            # åŠ è½½æ•°æ®
            data_info = self.load_tas_file(file_path)
            if data_info is None:
                print("âŒ åŠ è½½å¤±è´¥")
                continue
            
            print(f"   æ•°æ®å½¢çŠ¶: {data_info['shape']} (æ³¢é•¿Ã—æ—¶é—´)")
            print(f"   æ³¢é•¿èŒƒå›´: {data_info['wavelengths'].min():.1f} - {data_info['wavelengths'].max():.1f} nm")
            print(f"   æ—¶é—´èŒƒå›´: {data_info['time_delays'].min():.2f} - {data_info['time_delays'].max():.2f} ps")
            
            # åˆ†æä¸‰ç±»ç‰¹å¾
            multi_peak = self.analyze_multi_peak_overlap(data_info)
            transient = self.analyze_transient_decay(data_info)
            low_snr = self.analyze_low_snr(data_info)
            
            analyses = {
                'multi_peak': multi_peak,
                'transient': transient,
                'low_snr': low_snr
            }
            
            # åˆ†ç±»ç»“æœ
            categories = []
            
            if multi_peak['is_multi_peak']:
                categories.append('multi_peak_overlap')
                print(f"   âœ… å¤šå³°é‡å : è¯„åˆ†={multi_peak['score']:.3f}, å³°æ•°={multi_peak['max_peaks']}")
            
            if transient['is_transient']:
                categories.append('transient_decay')
                print(f"   âœ… ç¬æ€è¡°å‡: RÂ²={transient['score']:.3f}, Ï„å¹³å‡={transient['avg_time_constant']:.2f}ps")
            
            if low_snr['is_low_snr']:
                categories.append('low_snr')
                print(f"   âœ… ä½ä¿¡å™ªæ¯”: SNR={low_snr['snr']:.2f}")
            
            if not categories:
                print(f"   â– å¸¸è§„æ•°æ® (SNR={low_snr['snr']:.2f})")
            
            # ä¿å­˜ç»“æœ
            file_result = {
                'file_name': Path(file_path).name,
                'file_path': file_path,
                'shape': data_info['shape'],
                'wavelength_range': [float(data_info['wavelengths'].min()), float(data_info['wavelengths'].max())],
                'time_range': [float(data_info['time_delays'].min()), float(data_info['time_delays'].max())],
                'categories': categories,
                'analyses': self._serialize_analyses(analyses)
            }
            
            # ä¸ºæ¯ä¸ªç±»åˆ«ä¿å­˜
            for category in categories:
                self.results[category].append(file_result)
                
                # åˆ›å»ºç±»åˆ«ç›®å½•
                category_dir = self.output_root / category
                category_dir.mkdir(exist_ok=True)
                
                # ç”Ÿæˆå¯è§†åŒ–
                safe_name = Path(file_path).stem.replace(' ', '_').replace('-', '_')
                viz_file = category_dir / f"{safe_name}.png"
                
                try:
                    self.create_comprehensive_visualization(data_info, analyses, viz_file)
                    print(f"     ğŸ’¾ {category} å¯è§†åŒ–: {viz_file.relative_to(self.output_root)}")
                except Exception as e:
                    print(f"     âŒ å¯è§†åŒ–å¤±è´¥: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        self.save_results()
    
    def _serialize_analyses(self, analyses):
        """åºåˆ—åŒ–åˆ†æç»“æœä»¥ä¾¿JSONä¿å­˜"""
        def convert(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert(item) for item in obj]
            return obj
        
        return convert(analyses)
    
    def generate_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        report_file = self.output_root / "TASæ•°æ®ç­›é€‰æŠ¥å‘Š.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# TASæ•°æ®ç­›é€‰æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç»Ÿè®¡
            f.write("## ğŸ“Š ç­›é€‰ç»Ÿè®¡\n\n")
            f.write(f"- **å¤šå³°é‡å æ•°æ®**: {len(self.results['multi_peak_overlap'])} ä¸ª\n")
            f.write(f"- **ç¬æ€è¡°å‡æ•°æ®**: {len(self.results['transient_decay'])} ä¸ª\n")
            f.write(f"- **ä½ä¿¡å™ªæ¯”æ•°æ®**: {len(self.results['low_snr'])} ä¸ª\n\n")
            
            # è¯¦ç»†ç»“æœ
            for category, title in [
                ('multi_peak_overlap', 'å¤šå³°é‡å æ•°æ®'),
                ('transient_decay', 'ç¬æ€è¡°å‡æ•°æ®'),
                ('low_snr', 'ä½ä¿¡å™ªæ¯”æ•°æ®')
            ]:
                f.write(f"## ğŸ¯ {title}\n\n")
                
                if self.results[category]:
                    for i, item in enumerate(self.results[category], 1):
                        f.write(f"### {i}. {item['file_name']}\n\n")
                        f.write(f"- **æ–‡ä»¶è·¯å¾„**: `{item['file_path']}`\n")
                        f.write(f"- **æ•°æ®å½¢çŠ¶**: {item['shape'][0]}Ã—{item['shape'][1]} (æ³¢é•¿Ã—æ—¶é—´)\n")
                        f.write(f"- **æ³¢é•¿èŒƒå›´**: {item['wavelength_range'][0]:.1f} - {item['wavelength_range'][1]:.1f} nm\n")
                        f.write(f"- **æ—¶é—´èŒƒå›´**: {item['time_range'][0]:.2f} - {item['time_range'][1]:.2f} ps\n")
                        
                        # ç‰¹å¾åˆ†æç»“æœ
                        analyses = item['analyses']
                        if category == 'multi_peak_overlap':
                            mp = analyses['multi_peak']
                            f.write(f"- **é‡å è¯„åˆ†**: {mp['score']:.3f}\n")
                            f.write(f"- **æœ€å¤§å³°æ•°**: {mp['max_peaks']}\n")
                            f.write(f"- **é‡å æ—¶é—´ç‚¹æ•°**: {mp['num_overlap_times']}\n")
                        elif category == 'transient_decay':
                            td = analyses['transient']
                            f.write(f"- **å¹³å‡æ‹ŸåˆRÂ²**: {td['score']:.3f}\n")
                            f.write(f"- **å¹³å‡æ—¶é—´å¸¸æ•°**: {td['avg_time_constant']:.2f} ps\n")
                            f.write(f"- **è¡°å‡æ³¢é•¿æ•°**: {td['num_decay_wavelengths']}\n")
                        elif category == 'low_snr':
                            ls = analyses['low_snr']
                            f.write(f"- **ä¿¡å™ªæ¯”**: {ls['snr']:.2f}\n")
                            f.write(f"- **å™ªå£°æ°´å¹³**: {ls['noise_level']:.2e}\n")
                            f.write(f"- **ä¿¡å·å¼ºåº¦**: {ls['signal_strength']:.2e}\n")
                        
                        # å¯è§†åŒ–æ–‡ä»¶
                        safe_name = Path(item['file_path']).stem.replace(' ', '_').replace('-', '_')
                        viz_file = f"{category}/{safe_name}.png"
                        f.write(f"- **å¯è§†åŒ–æ–‡ä»¶**: {viz_file}\n\n")
                        f.write(f"![{title}å¯è§†åŒ–]({viz_file})\n\n")
                else:
                    f.write("æš‚æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®ã€‚\n\n")
            
            # ä½¿ç”¨å»ºè®®
            f.write("## ğŸ’¡ ä½¿ç”¨å»ºè®®\n\n")
            f.write("### å¤šå³°é‡å æ•°æ®\n")
            f.write("- æµ‹è¯•MCR-ALSåœ¨å¤æ‚å…‰è°±é‡å ä¸‹çš„ç»„åˆ†åˆ†è¾¨èƒ½åŠ›\n")
            f.write("- å»ºè®®ä½¿ç”¨æ›´å¤šç»„åˆ†æ•°å’Œä¸¥æ ¼çº¦æŸ\n")
            f.write("- å¯å°è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•\n\n")
            
            f.write("### ç¬æ€è¡°å‡æ•°æ®\n")
            f.write("- éªŒè¯MCR-ALSå¯¹æ—¶é—´åˆ†è¾¨åŠ¨åŠ›å­¦çš„åˆ†æèƒ½åŠ›\n")
            f.write("- å…³æ³¨æ—¶é—´å¸¸æ•°çš„ç‰©ç†åˆç†æ€§\n")
            f.write("- å¯ç»“åˆå·²çŸ¥åŠ¨åŠ›å­¦æ¨¡å‹éªŒè¯\n\n")
            
            f.write("### ä½ä¿¡å™ªæ¯”æ•°æ®\n")
            f.write("- æµ‹è¯•ç®—æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§\n")
            f.write("- å»ºè®®å¢åŠ éšæœºåˆå§‹åŒ–æ¬¡æ•°\n")
            f.write("- å¯å°è¯•é¢„å¤„ç†é™å™ªæŠ€æœ¯\n\n")
        
        print(f"\nğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_file = self.output_root / "screening_results.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ TASæŒ‘æˆ˜æ€§æ•°æ®ç­›é€‰å™¨ - ç®€åŒ–ç‰ˆ")
    print("=" * 50)
    
    screener = SimpleTASScreener()
    screener.run_screening()
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ğŸ‰ ç­›é€‰å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {screener.output_root}")
    print(f"ğŸ¯ å¤šå³°é‡å æ•°æ®: {len(screener.results['multi_peak_overlap'])} ä¸ª")
    print(f"âš¡ ç¬æ€è¡°å‡æ•°æ®: {len(screener.results['transient_decay'])} ä¸ª")
    print(f"ğŸ“‰ ä½ä¿¡å™ªæ¯”æ•°æ®: {len(screener.results['low_snr'])} ä¸ª")
    
    # æ˜¾ç¤ºè¾“å‡ºç›®å½•ç»“æ„
    print(f"\nğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
    for item in screener.output_root.iterdir():
        if item.is_dir():
            file_count = len(list(item.glob('*.png')))
            print(f"  ğŸ“ {item.name}/ ({file_count} ä¸ªå¯è§†åŒ–æ–‡ä»¶)")
        else:
            print(f"  ğŸ“„ {item.name}")

if __name__ == "__main__":
    main()