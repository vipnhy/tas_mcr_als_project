#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤šè½®MCR-ALSæ‰¹é‡åˆ†æå™¨
å¯¹ç­›é€‰å‡ºçš„ä¸‰ç±»æŒ‘æˆ˜æ€§æ•°æ®æ‰§è¡ŒMCR-ALSåˆ†æå¹¶æ±‡æ€»ç»“æœ
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import pickle
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥MCR-ALSç›¸å…³æ¨¡å—
import sys
sys.path.append('.')
from mcr.mcr_als import MCRALS
try:
    from mcr.constraints import ConstraintUnimodality, ConstraintNNLS, ConstraintNormalize
except ImportError:
    # ä½¿ç”¨ç®€åŒ–çš„çº¦æŸå®ç°
    ConstraintUnimodality = None
    ConstraintNNLS = None
    ConstraintNormalize = None

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MCRALSBatchAnalyzer:
    """å¤šè½®MCR-ALSæ‰¹é‡åˆ†æå™¨"""
    
    def __init__(self, output_dir="experiments/results/mcr_als_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®ç±»åˆ«å’Œå¯¹åº”çš„MCRå‚æ•°é…ç½®
        self.category_configs = {
            'multi_peak_overlap': {
                'n_components': [2, 3, 4],  # å¤šå³°é‡å é€šå¸¸éœ€è¦å¤šä¸ªç»„åˆ†
                'max_iter': 150,
                'constraints': ['nnls', 'unimodal_c', 'normalize'],
                'description': 'å¤šå³°é‡å æ•°æ® - é€‚åˆæ£€æµ‹å¤æ‚å…‰è°±é‡å '
            },
            'transient_decay': {
                'n_components': [2, 3],  # ç¬æ€è¡°å‡é€šå¸¸2-3ä¸ªç»„åˆ†è¶³å¤Ÿ
                'max_iter': 100,
                'constraints': ['nnls', 'normalize'],
                'description': 'ç¬æ€è¡°å‡æ•°æ® - é€‚åˆæ£€æµ‹åŠ¨åŠ›å­¦è¿‡ç¨‹'
            },
            'low_snr': {
                'n_components': [2, 3],  # ä½ä¿¡å™ªæ¯”æ•°æ®ä¿å®ˆä¼°è®¡ç»„åˆ†æ•°
                'max_iter': 200,  # å¢åŠ è¿­ä»£æ¬¡æ•°æé«˜æ”¶æ•›æ€§
                'constraints': ['nnls', 'normalize'],
                'description': 'ä½ä¿¡å™ªæ¯”æ•°æ® - éœ€è¦æ›´å¤šè¿­ä»£å’Œçº¦æŸ'
            }
        }
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.results = {
            'multi_peak_overlap': [],
            'transient_decay': [],
            'low_snr': [],
            'summary': {}
        }
        
    def load_screening_results(self, screening_file="experiments/results/data_screening/screening_results.json"):
        """åŠ è½½ç­›é€‰ç»“æœ"""
        try:
            with open(screening_file, 'r', encoding='utf-8') as f:
                screening_data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½ç­›é€‰ç»“æœ: {screening_file}")
            return screening_data
        except FileNotFoundError:
            print(f"âŒ ç­›é€‰ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: {screening_file}")
            return None
        except Exception as e:
            print(f"âŒ åŠ è½½ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return None
    
    def load_tas_data(self, file_path):
        """åŠ è½½TASæ•°æ®æ–‡ä»¶"""
        try:
            # è¯»å–CSVæ–‡ä»¶ï¼Œä½¿ç”¨è‡ªå®šä¹‰å¤„ç†
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            if len(lines) < 2:
                print(f"   âŒ æ•°æ®æ–‡ä»¶å¤ªçŸ­: {len(lines)} è¡Œ")
                return None
            
            # ç¬¬ä¸€è¡Œæ˜¯æ—¶é—´å»¶è¿Ÿ
            time_line = lines[0].strip().split(',')
            time_delays = []
            for i, t_str in enumerate(time_line):
                if i == 0:  # ç¬¬ä¸€åˆ—æ˜¯0.0ï¼Œè·³è¿‡
                    continue
                try:
                    time_delays.append(float(t_str))
                except:
                    time_delays.append(0.0)
            
            time_delays = np.array(time_delays)
            
            # å‰©ä½™è¡Œæ˜¯æ³¢é•¿å’Œæ•°æ®
            wavelengths = []
            data_rows = []
            
            for line in lines[1:]:
                if not line.strip():
                    continue
                    
                parts = line.strip().split(',')
                if len(parts) < 2:
                    continue
                
                try:
                    # ç¬¬ä¸€åˆ—æ˜¯æ³¢é•¿
                    wl = float(parts[0])
                    wavelengths.append(wl)
                    
                    # å…¶ä½™åˆ—æ˜¯æ•°æ®
                    row_data = []
                    for val_str in parts[1:]:
                        try:
                            # å¤„ç†ç‰¹æ®Šå€¼
                            if val_str in ['Inf', '+Inf']:
                                val = 1e10
                            elif val_str in ['-Inf']:
                                val = -1e10
                            elif val_str in ['NaN', 'nan']:
                                val = 0.0
                            else:
                                val = float(val_str)
                                if not np.isfinite(val):
                                    val = 0.0
                            row_data.append(val)
                        except:
                            row_data.append(0.0)
                    
                    # ç¡®ä¿é•¿åº¦åŒ¹é…
                    while len(row_data) < len(time_delays):
                        row_data.append(0.0)
                    row_data = row_data[:len(time_delays)]
                    
                    data_rows.append(row_data)
                    
                except Exception as e:
                    print(f"   âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                    continue
            
            if len(data_rows) < 10:
                print(f"   âŒ æ•°æ®è¡Œæ•°å¤ªå°‘: {len(data_rows)}")
                return None
            
            wavelengths = np.array(wavelengths)
            data = np.array(data_rows)
            
            # æ•°æ®éªŒè¯å’Œæ¸…ç†
            if data.shape[0] < 10 or data.shape[1] < 10:
                print(f"   âŒ æ•°æ®å½¢çŠ¶å¤ªå°: {data.shape}")
                return None
            
            # å¤„ç†å¼‚å¸¸å€¼
            data = np.where(np.isfinite(data), data, 0)
            data = np.where(np.abs(data) > 1e10, 0, data)  # é™åˆ¶æå¤§å€¼
            
            print(f"   âœ… æ•°æ®åŠ è½½æˆåŠŸ: {data.shape} (æ³¢é•¿Ã—æ—¶é—´)")
            print(f"   æ³¢é•¿èŒƒå›´: {wavelengths.min():.1f} - {wavelengths.max():.1f} nm")
            print(f"   æ—¶é—´èŒƒå›´: {time_delays.min():.2f} - {time_delays.max():.2f} ps")
            print(f"   æ•°æ®èŒƒå›´: {data.min():.2e} - {data.max():.2e}")
            
            return {
                'data': data,
                'wavelengths': wavelengths,
                'time_delays': time_delays,
                'shape': data.shape
            }
            
        except Exception as e:
            print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def setup_mcr_constraints(self, constraint_names):
        """è®¾ç½®MCRçº¦æŸæ¡ä»¶"""
        # è¿”å›çº¦æŸåç§°åˆ—è¡¨ï¼Œå°†åœ¨MCRåˆå§‹åŒ–æ—¶ä½¿ç”¨
        return constraint_names
    
    def run_mcr_analysis(self, data_info, category, n_components, config):
        """è¿è¡Œå•æ¬¡MCR-ALSåˆ†æ"""
        data = data_info['data']
        
        try:
            # åˆå§‹åŒ–MCR-ALS
            mcr = MCRALS(
                n_components=n_components,
                max_iter=config['max_iter'],
                tol=1e-6
            )
            
            # è¿è¡ŒMCR-ALSåˆ†æ
            mcr.fit(data.T)  # è½¬ç½®æ•°æ®ï¼ŒMCRæœŸæœ›æ—¶é—´Ã—æ³¢é•¿æ ¼å¼
            
            # è·å–ç»“æœ
            C = mcr.C_opt_  # æµ“åº¦çŸ©é˜µ (æ—¶é—´Ã—ç»„åˆ†)
            S = mcr.S_opt_  # å…‰è°±çŸ©é˜µ (æ³¢é•¿Ã—ç»„åˆ†)
            
            # é‡æ„æ•°æ®
            reconstructed = C @ S.T
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            residuals = data.T - reconstructed
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((data.T - np.mean(data.T))**2)
            r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
            lof = mcr.lof_[-1] if mcr.lof_ else 100  # ä½¿ç”¨MCRå†…éƒ¨è®¡ç®—çš„LOF
            
            result = {
                'n_components': n_components,
                'converged': lof < 10.0,  # ç®€å•çš„æ”¶æ•›æ ‡å‡†
                'n_iterations': len(mcr.lof_),
                'r2': float(r2),
                'lof': float(lof),
                'concentration_profiles': C.T,  # è½¬ç½®ä¸ºç»„åˆ†Ã—æ—¶é—´æ ¼å¼
                'pure_spectra': S.T,  # è½¬ç½®ä¸ºç»„åˆ†Ã—æ³¢é•¿æ ¼å¼
                'constraints_used': config['constraints'],
                'final_d_augmented': reconstructed
            }
            
            print(f"     âœ… MCRåˆ†æå®Œæˆ: {n_components}ç»„åˆ†, RÂ²={r2:.3f}, LOF={lof:.2f}%")
            return result
            
        except Exception as e:
            print(f"     âŒ MCRåˆ†æå¤±è´¥: {e}")
            return None
    
    def analyze_single_file(self, file_path, category, item_info=None):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {Path(file_path).name}")
        
        # ä»item_infoè·å–å…‰è°±ç±»å‹ä¿¡æ¯
        spectrum_type = 'VIS'
        if item_info:
            spectrum_type = item_info.get('spectrum_type', 'VIS')
            print(f"   ğŸŒˆ å…‰è°±ç±»å‹: {spectrum_type}")
        
        # åŠ è½½æ•°æ®
        data_info = self.load_tas_data(file_path)
        if data_info is None:
            return None
        
        # æ·»åŠ å…‰è°±ç±»å‹ä¿¡æ¯åˆ°data_infoä¸­
        data_info['spectrum_type'] = spectrum_type
        if item_info:
            data_info['original_wavelength_range'] = item_info.get('original_wavelength_range', 'N/A')
            data_info['cropped_wavelength_range'] = item_info.get('cropped_wavelength_range', 'N/A')
        
        # è·å–é…ç½®
        config = self.category_configs[category]
        print(f"   ğŸ”§ ä½¿ç”¨é…ç½®: {config['description']}")
        
        file_results = {
            'file_path': str(file_path),
            'file_name': Path(file_path).name,
            'category': category,
            'spectrum_type': spectrum_type,
            'data_shape': data_info['shape'],
            'wavelength_range': [float(data_info['wavelengths'].min()), 
                               float(data_info['wavelengths'].max())],
            'time_range': [float(data_info['time_delays'].min()), 
                          float(data_info['time_delays'].max())],
            'mcr_results': [],
            'best_result': None
        }
        
        # å¯¹ä¸åŒç»„åˆ†æ•°è¿›è¡ŒMCRåˆ†æ
        best_r2 = -1
        for n_comp in config['n_components']:
            print(f"   ğŸ§ª å°è¯• {n_comp} ä¸ªç»„åˆ†...")
            result = self.run_mcr_analysis(data_info, category, n_comp, config)
            
            if result is not None:
                file_results['mcr_results'].append(result)
                
                # é€‰æ‹©æœ€ä½³ç»“æœï¼ˆåŸºäºRÂ²ï¼‰
                if result['r2'] > best_r2:
                    best_r2 = result['r2']
                    file_results['best_result'] = result
        
        # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
        self.save_single_file_results(file_results, data_info)
        
        return file_results
    
    def save_single_file_results(self, file_results, data_info):
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ"""
        file_name = Path(file_results['file_name']).stem
        category = file_results['category']
        
        # åˆ›å»ºæ–‡ä»¶ä¸“ç”¨ç›®å½•
        file_dir = self.output_dir / category / file_name
        file_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€ä½³MCRç»“æœ
        if file_results['best_result'] is not None:
            best = file_results['best_result']
            
            # ä¿å­˜æµ“åº¦è½®å»“
            np.savetxt(file_dir / 'concentration_profiles.csv',
                      best['concentration_profiles'], delimiter=',')
            
            # ä¿å­˜çº¯å…‰è°±
            np.savetxt(file_dir / 'pure_spectra.csv',
                      best['pure_spectra'], delimiter=',')
            
            # åˆ›å»ºå¯è§†åŒ–
            self.create_mcr_visualization(data_info, best, file_dir)
        
        # ä¿å­˜å®Œæ•´ç»“æœä¸ºJSON
        results_copy = file_results.copy()
        # ç§»é™¤numpyæ•°ç»„ï¼ˆä¸èƒ½JSONåºåˆ—åŒ–ï¼‰
        for result in results_copy['mcr_results']:
            for key in ['concentration_profiles', 'pure_spectra', 'final_d_augmented']:
                if key in result:
                    del result[key]
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            for key, value in result.items():
                if isinstance(value, (np.bool_, bool)):
                    result[key] = bool(value)
                elif isinstance(value, (np.integer, np.int64)):
                    result[key] = int(value)
                elif isinstance(value, (np.floating, np.float64)):
                    result[key] = float(value)
        
        if results_copy['best_result']:
            for key in ['concentration_profiles', 'pure_spectra', 'final_d_augmented']:
                if key in results_copy['best_result']:
                    del results_copy['best_result'][key]
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            for key, value in results_copy['best_result'].items():
                if isinstance(value, (np.bool_, bool)):
                    results_copy['best_result'][key] = bool(value)
                elif isinstance(value, (np.integer, np.int64)):
                    results_copy['best_result'][key] = int(value)
                elif isinstance(value, (np.floating, np.float64)):
                    results_copy['best_result'][key] = float(value)
        
        with open(file_dir / 'analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(results_copy, f, indent=2, ensure_ascii=False)
    
    def create_mcr_visualization(self, data_info, mcr_result, output_dir):
        """åˆ›å»ºMCRåˆ†æå¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        data = data_info['data']
        wavelengths = data_info['wavelengths']
        time_delays = data_info['time_delays']
        
        # æ ¹æ®å…‰è°±ç±»å‹ç¡®å®šåæ ‡è½´èŒƒå›´
        spectrum_type = data_info.get('spectrum_type', 'VIS')
        if spectrum_type == 'UV':
            wl_range = (380, 650)
        elif spectrum_type == 'NIR':
            wl_range = (1100, 1620)
        else:  # VIS or default
            wl_range = (500, 950)
        
        # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´èŒƒå›´
        actual_min, actual_max = wavelengths.min(), wavelengths.max()
        wl_range = (max(wl_range[0], actual_min), min(wl_range[1], actual_max))
        
        # 1. åŸå§‹æ•°æ®çƒ­å›¾
        im1 = axes[0, 0].imshow(data, aspect='auto', cmap='RdBu_r',
                               extent=[time_delays.min(), time_delays.max(),
                                      wavelengths.max(), wavelengths.min()])
        axes[0, 0].set_title(f'åŸå§‹æ•°æ® ({spectrum_type})')
        axes[0, 0].set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        axes[0, 0].set_ylabel('æ³¢é•¿ (nm)')
        axes[0, 0].set_ylim(wl_range[1], wl_range[0])  # åè½¬Yè½´ï¼Œä¸Šå¤§ä¸‹å°
        plt.colorbar(im1, ax=axes[0, 0])
        
        # 2. é‡æ„æ•°æ®çƒ­å›¾
        reconstructed = mcr_result['concentration_profiles'].T @ mcr_result['pure_spectra']
        im2 = axes[0, 1].imshow(reconstructed.T, aspect='auto', cmap='RdBu_r',
                               extent=[time_delays.min(), time_delays.max(),
                                      wavelengths.max(), wavelengths.min()])
        axes[0, 1].set_title(f'MCRé‡æ„ (RÂ²={mcr_result["r2"]:.3f})')
        axes[0, 1].set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        axes[0, 1].set_ylabel('æ³¢é•¿ (nm)')
        axes[0, 1].set_ylim(wl_range[1], wl_range[0])  # åè½¬Yè½´ï¼Œä¸Šå¤§ä¸‹å°
        plt.colorbar(im2, ax=axes[0, 1])
        
        # 3. æ®‹å·®çƒ­å›¾
        residuals = data - reconstructed.T
        im3 = axes[0, 2].imshow(residuals, aspect='auto', cmap='RdBu_r',
                               extent=[time_delays.min(), time_delays.max(),
                                      wavelengths.max(), wavelengths.min()])
        axes[0, 2].set_title(f'æ®‹å·® (LOF={mcr_result["lof"]:.2f}%)')
        axes[0, 2].set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        axes[0, 2].set_ylabel('æ³¢é•¿ (nm)')
        axes[0, 2].set_ylim(wl_range[1], wl_range[0])  # åè½¬Yè½´ï¼Œä¸Šå¤§ä¸‹å°
        plt.colorbar(im3, ax=axes[0, 2])
        
        # 4. æµ“åº¦è½®å»“
        for i in range(mcr_result['n_components']):
            axes[1, 0].plot(time_delays, mcr_result['concentration_profiles'][i, :],
                           label=f'ç»„åˆ† {i+1}', linewidth=2)
        axes[1, 0].set_title('æµ“åº¦è½®å»“')
        axes[1, 0].set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        axes[1, 0].set_ylabel('æµ“åº¦')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. çº¯å…‰è°±
        for i in range(mcr_result['n_components']):
            axes[1, 1].plot(wavelengths, mcr_result['pure_spectra'][i, :],
                           label=f'ç»„åˆ† {i+1}', linewidth=2)
        axes[1, 1].set_title(f'çº¯å…‰è°± ({spectrum_type})')
        axes[1, 1].set_xlabel('æ³¢é•¿ (nm)')
        axes[1, 1].set_ylabel('å¸æ”¶å¼ºåº¦')
        axes[1, 1].set_xlim(wl_range[0], wl_range[1])  # è®¾ç½®Xè½´èŒƒå›´
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. æ‹Ÿåˆè´¨é‡æŒ‡æ ‡
        metrics_text = f"""
MCR-ALS åˆ†æç»“æœ

å…‰è°±ç±»å‹: {spectrum_type}
ç»„åˆ†æ•°é‡: {mcr_result['n_components']}
è¿­ä»£æ¬¡æ•°: {mcr_result['n_iterations']}
æ˜¯å¦æ”¶æ•›: {'æ˜¯' if mcr_result['converged'] else 'å¦'}

æ‹Ÿåˆè´¨é‡:
RÂ² = {mcr_result['r2']:.4f}
LOF = {mcr_result['lof']:.2f}%

ä½¿ç”¨çš„çº¦æŸ:
{', '.join(mcr_result['constraints_used'])}
        """
        axes[1, 2].text(0.1, 0.5, metrics_text, transform=axes[1, 2].transAxes,
                        fontsize=12, verticalalignment='center',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        axes[1, 2].set_xlim(0, 1)
        axes[1, 2].set_ylim(0, 1)
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'mcr_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"     ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜: {output_dir / 'mcr_analysis.png'}")
    
    def run_batch_analysis(self, max_files_per_category=5):
        """è¿è¡Œæ‰¹é‡åˆ†æ"""
        print("ğŸš€ å¼€å§‹å¤šè½®MCR-ALSæ‰¹é‡åˆ†æ")
        print("=" * 60)
        
        # åŠ è½½ç­›é€‰ç»“æœ
        screening_data = self.load_screening_results()
        if screening_data is None:
            print("âŒ æ— æ³•ç»§ç»­åˆ†æï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ç­›é€‰")
            return
        
        # åˆ†ææ¯ä¸ªç±»åˆ«çš„æ•°æ®
        for category in ['multi_peak_overlap', 'transient_decay', 'low_snr']:
            if category not in screening_data:
                print(f"âš ï¸ ç±»åˆ« {category} æœªæ‰¾åˆ°ç­›é€‰æ•°æ®")
                continue
            
            category_data = screening_data[category]
            print(f"\nğŸ¯ åˆ†æç±»åˆ«: {category}")
            print(f"ğŸ“ å…±æ‰¾åˆ° {len(category_data)} ä¸ªæ–‡ä»¶")
            
            # é™åˆ¶åˆ†ææ–‡ä»¶æ•°é‡
            files_to_analyze = category_data[:max_files_per_category]
            if len(files_to_analyze) < len(category_data):
                print(f"ğŸ“‹ ä¸ºäº†æ•ˆç‡ï¼Œåªåˆ†æå‰ {max_files_per_category} ä¸ªæ–‡ä»¶")
            
            category_results = []
            
            for i, file_info in enumerate(files_to_analyze, 1):
                print(f"\n[{i}/{len(files_to_analyze)}] å¤„ç†: {category}")
                
                file_path = file_info['file_path']
                result = self.analyze_single_file(file_path, category, file_info)
                
                if result is not None:
                    category_results.append(result)
            
            self.results[category] = category_results
            print(f"âœ… {category} ç±»åˆ«åˆ†æå®Œæˆ: {len(category_results)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_summary_report()
        
        print(f"\nğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        
        # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
        summary = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_files_analyzed': 0,
            'category_statistics': {},
            'overall_statistics': {
                'avg_r2': 0,
                'avg_lof': 0,
                'convergence_rate': 0
            }
        }
        
        all_r2_values = []
        all_lof_values = []
        all_convergence = []
        
        for category, results in self.results.items():
            if not results:
                continue
            
            category_r2 = []
            category_lof = []
            category_convergence = []
            
            for result in results:
                if result['best_result'] is not None:
                    best = result['best_result']
                    category_r2.append(best['r2'])
                    category_lof.append(best['lof'])
                    category_convergence.append(best['converged'])
            
            if category_r2:
                summary['category_statistics'][category] = {
                    'files_count': len(results),
                    'avg_r2': float(np.mean(category_r2)),
                    'std_r2': float(np.std(category_r2)),
                    'avg_lof': float(np.mean(category_lof)),
                    'std_lof': float(np.std(category_lof)),
                    'convergence_rate': float(np.mean(category_convergence))
                }
                
                all_r2_values.extend(category_r2)
                all_lof_values.extend(category_lof)
                all_convergence.extend(category_convergence)
        
        # æ€»ä½“ç»Ÿè®¡
        if all_r2_values:
            summary['total_files_analyzed'] = len(all_r2_values)
            summary['overall_statistics'] = {
                'avg_r2': float(np.mean(all_r2_values)),
                'std_r2': float(np.std(all_r2_values)),
                'avg_lof': float(np.mean(all_lof_values)),
                'std_lof': float(np.std(all_lof_values)),
                'convergence_rate': float(np.mean(all_convergence))
            }
        else:
            summary['overall_statistics'] = {
                'avg_r2': 0.0,
                'std_r2': 0.0,
                'avg_lof': 100.0,
                'std_lof': 0.0,
                'convergence_rate': 0.0
            }
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        with open(self.output_dir / 'batch_analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ±‡æ€»å¯è§†åŒ–
        self.create_summary_visualization(summary)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(summary)
        
        print("âœ… æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    def create_summary_visualization(self, summary):
        """åˆ›å»ºæ±‡æ€»å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        categories = list(summary['category_statistics'].keys())
        if not categories:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨æ•°æ®ç”Ÿæˆå¯è§†åŒ–")
            return
        
        # 1. RÂ²æ¯”è¾ƒ
        r2_values = [summary['category_statistics'][cat]['avg_r2'] for cat in categories]
        r2_errors = [summary['category_statistics'][cat]['std_r2'] for cat in categories]
        
        axes[0, 0].bar(categories, r2_values, yerr=r2_errors, capsize=5, alpha=0.7)
        axes[0, 0].set_title('å„ç±»åˆ«å¹³å‡RÂ²å€¼')
        axes[0, 0].set_ylabel('RÂ²')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. LOFæ¯”è¾ƒ
        lof_values = [summary['category_statistics'][cat]['avg_lof'] for cat in categories]
        lof_errors = [summary['category_statistics'][cat]['std_lof'] for cat in categories]
        
        axes[0, 1].bar(categories, lof_values, yerr=lof_errors, capsize=5, alpha=0.7, color='orange')
        axes[0, 1].set_title('å„ç±»åˆ«å¹³å‡LOFå€¼')
        axes[0, 1].set_ylabel('LOF (%)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. æ”¶æ•›ç‡æ¯”è¾ƒ
        convergence_rates = [summary['category_statistics'][cat]['convergence_rate'] * 100 
                           for cat in categories]
        
        axes[1, 0].bar(categories, convergence_rates, alpha=0.7, color='green')
        axes[1, 0].set_title('å„ç±»åˆ«æ”¶æ•›ç‡')
        axes[1, 0].set_ylabel('æ”¶æ•›ç‡ (%)')
        axes[1, 0].set_ylim(0, 100)
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æ–‡ä»¶æ•°é‡ç»Ÿè®¡
        file_counts = [summary['category_statistics'][cat]['files_count'] for cat in categories]
        
        axes[1, 1].pie(file_counts, labels=categories, autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('å„ç±»åˆ«åˆ†ææ–‡ä»¶æ•°é‡åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'batch_analysis_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ’¾ æ±‡æ€»å¯è§†åŒ–å·²ä¿å­˜: {self.output_dir / 'batch_analysis_summary.png'}")
    
    def generate_markdown_report(self, summary):
        """ç”ŸæˆMarkdownæ±‡æ€»æŠ¥å‘Š"""
        report_path = self.output_dir / 'MCR_ALS_æ‰¹é‡åˆ†ææŠ¥å‘Š.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# MCR-ALS æ‰¹é‡åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {summary['analysis_time']}\n\n")
            f.write(f"**æ€»åˆ†ææ–‡ä»¶æ•°**: {summary['total_files_analyzed']}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            overall = summary['overall_statistics']
            f.write(f"- **å¹³å‡ RÂ²**: {overall['avg_r2']:.4f} Â± {overall['std_r2']:.4f}\n")
            f.write(f"- **å¹³å‡ LOF**: {overall['avg_lof']:.2f}% Â± {overall['std_lof']:.2f}%\n")
            f.write(f"- **æ”¶æ•›ç‡**: {overall['convergence_rate']*100:.1f}%\n\n")
            
            # å„ç±»åˆ«è¯¦ç»†ç»Ÿè®¡
            f.write("## ğŸ¯ å„ç±»åˆ«è¯¦ç»†ç»Ÿè®¡\n\n")
            
            for category, stats in summary['category_statistics'].items():
                category_name = {
                    'multi_peak_overlap': 'å¤šå³°é‡å æ•°æ®',
                    'transient_decay': 'ç¬æ€è¡°å‡æ•°æ®',
                    'low_snr': 'ä½ä¿¡å™ªæ¯”æ•°æ®'
                }.get(category, category)
                
                f.write(f"### {category_name}\n\n")
                f.write(f"- **åˆ†ææ–‡ä»¶æ•°**: {stats['files_count']}\n")
                f.write(f"- **å¹³å‡ RÂ²**: {stats['avg_r2']:.4f} Â± {stats['std_r2']:.4f}\n")
                f.write(f"- **å¹³å‡ LOF**: {stats['avg_lof']:.2f}% Â± {stats['std_lof']:.2f}%\n")
                f.write(f"- **æ”¶æ•›ç‡**: {stats['convergence_rate']*100:.1f}%\n\n")
            
            # é…ç½®ä¿¡æ¯
            f.write("## âš™ï¸ åˆ†æé…ç½®\n\n")
            for category, config in self.category_configs.items():
                category_name = {
                    'multi_peak_overlap': 'å¤šå³°é‡å æ•°æ®',
                    'transient_decay': 'ç¬æ€è¡°å‡æ•°æ®',
                    'low_snr': 'ä½ä¿¡å™ªæ¯”æ•°æ®'
                }.get(category, category)
                
                f.write(f"### {category_name}\n")
                f.write(f"- **æè¿°**: {config['description']}\n")
                f.write(f"- **ç»„åˆ†æ•°é‡**: {config['n_components']}\n")
                f.write(f"- **æœ€å¤§è¿­ä»£æ•°**: {config['max_iter']}\n")
                f.write(f"- **çº¦æŸæ¡ä»¶**: {', '.join(config['constraints'])}\n\n")
            
            # å¯è§†åŒ–å›¾ç‰‡
            f.write("## ğŸ“ˆ æ±‡æ€»å¯è§†åŒ–\n\n")
            f.write("![æ‰¹é‡åˆ†ææ±‡æ€»](batch_analysis_summary.png)\n\n")
            
            # æ–‡ä»¶ç›®å½•ç»“æ„
            f.write("## ğŸ“ ç»“æœæ–‡ä»¶ç»“æ„\n\n")
            f.write("```\n")
            f.write("experiments/results/mcr_als_results/\n")
            f.write("â”œâ”€â”€ batch_analysis_summary.json  # æ±‡æ€»ç»Ÿè®¡æ•°æ®\n")
            f.write("â”œâ”€â”€ batch_analysis_summary.png   # æ±‡æ€»å¯è§†åŒ–å›¾è¡¨\n")
            f.write("â”œâ”€â”€ MCR_ALS_æ‰¹é‡åˆ†ææŠ¥å‘Š.md      # æœ¬æŠ¥å‘Š\n")
            f.write("â”œâ”€â”€ multi_peak_overlap/          # å¤šå³°é‡å æ•°æ®åˆ†æç»“æœ\n")
            f.write("â”œâ”€â”€ transient_decay/             # ç¬æ€è¡°å‡æ•°æ®åˆ†æç»“æœ\n")
            f.write("â””â”€â”€ low_snr/                     # ä½ä¿¡å™ªæ¯”æ•°æ®åˆ†æç»“æœ\n")
            f.write("    â””â”€â”€ [æ–‡ä»¶å]/\n")
            f.write("        â”œâ”€â”€ concentration_profiles.csv  # æµ“åº¦è½®å»“\n")
            f.write("        â”œâ”€â”€ pure_spectra.csv           # çº¯å…‰è°±\n")
            f.write("        â”œâ”€â”€ mcr_analysis.png           # å¯è§†åŒ–ç»“æœ\n")
            f.write("        â””â”€â”€ analysis_summary.json      # åˆ†ææ‘˜è¦\n")
            f.write("```\n\n")
        
        print(f"ğŸ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ MCR-ALSæ‰¹é‡åˆ†æå™¨")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MCRALSBatchAnalyzer()
    
    # è¿è¡Œæ‰¹é‡åˆ†æ
    analyzer.run_batch_analysis(max_files_per_category=3)  # æ¯ç±»åˆ«åˆ†æ3ä¸ªæ–‡ä»¶
    
    print("\nğŸ‰ æ‰€æœ‰åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()