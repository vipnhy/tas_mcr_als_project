#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çœŸå®TASæ•°æ®é¢„å¤„ç†æµ‹è¯•è„šæœ¬

ä½¿ç”¨é¡¹ç›®ä¸­çš„çœŸå®ç¬æ€å¸æ”¶å…‰è°±æ•°æ®æµ‹è¯•é¢„å¤„ç†æ¨¡å—åŠŸèƒ½
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
from matplotlib.colors import TwoSlopeNorm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
from preprocessing import (
    create_standard_pipeline, 
    create_gentle_pipeline,
    create_aggressive_pipeline,
    create_chirp_corrected_pipeline,
    create_comprehensive_pipeline,
    preprocess_tas_data,
    TASPreprocessingPipeline,
    ChirpCorrector
)

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
rcParams['axes.unicode_minus'] = False
rcParams['font.size'] = 10

def read_file_auto(file, inf_handle=False, wavelength_range=None, delay_range=None):
    """
    å‚è€ƒapi.pyçš„è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹å¹¶è¯»å–æ–‡ä»¶
    file: æ–‡ä»¶è·¯å¾„
    inf_handle: æ˜¯å¦å¤„ç†æ— ç©·å€¼
    wavelength_range: æ³¢é•¿èŒƒå›´
    delay_range: å»¶è¿Ÿæ—¶é—´èŒƒå›´
    """
    import pandas as pd
    import numpy as np
    
    detected_type = "raw"  # é»˜è®¤ç±»å‹
    
    try:
        print("å¼€å§‹æ–‡ä»¶ç±»å‹è‡ªåŠ¨æ£€æµ‹...")
        # å°è¯•å¤šç§ç¼–ç æ–¹å¼è¯»å–æ–‡ä»¶
        encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        file_content = None
        used_encoding = None
        
        for encoding in encodings_to_try:
            try:
                print(f"å°è¯•ä½¿ç”¨ç¼–ç  {encoding} è¯»å–æ–‡ä»¶: {file}")
                with open(file, 'r', encoding=encoding) as f:
                    first_line = f.readline().strip()
                    second_line = f.readline().strip()
                    third_line = f.readline().strip()
                    file_content = (first_line, second_line, third_line)
                    used_encoding = encoding
                    print(f"âœ… æˆåŠŸä½¿ç”¨ç¼–ç  {encoding} è¯»å–æ–‡ä»¶")
                    break
            except UnicodeDecodeError as e:
                print(f"ç¼–ç  {encoding} è¯»å–å¤±è´¥: {str(e)}")
                continue
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                continue
        
        if file_content is None:
            print("âš ï¸ æ–‡ä»¶ç¼–ç æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ raw ç±»å‹")
            detected_type = "raw"
        else:
            first_line, second_line, third_line = file_content
            print(f"âœ… ä½¿ç”¨ç¼–ç : {used_encoding}")
            tab_char = '\t'
            comma_char = ','
            print(f"æ–‡ä»¶å†…å®¹åˆ†æ - ç¬¬1è¡Œé•¿åº¦: {len(first_line)}, åˆ¶è¡¨ç¬¦æ•°: {first_line.count(tab_char)}, é€—å·æ•°: {first_line.count(comma_char)}")
            print(f"æ–‡ä»¶å†…å®¹åˆ†æ - ç¬¬2è¡Œé•¿åº¦: {len(second_line)}, åˆ¶è¡¨ç¬¦æ•°: {second_line.count(tab_char)}, é€—å·æ•°: {second_line.count(comma_char)}")
            
            # æ›´ç²¾ç¡®çš„åˆ†éš”ç¬¦æ£€æµ‹é€»è¾‘
            tab_count = first_line.count(tab_char) + second_line.count(tab_char)
            comma_count = first_line.count(comma_char) + second_line.count(comma_char)
            
            # å¦‚æœåˆ¶è¡¨ç¬¦æ˜æ˜¾å¤šäºé€—å·ï¼Œåˆ¤å®šä¸ºhandleç±»å‹
            if tab_count > 0 and tab_count >= comma_count:
                detected_type = "handle"
                print(f"ğŸ¯ è‡ªåŠ¨æ£€æµ‹ï¼šæ–‡ä»¶ç±»å‹ä¸º handleï¼ˆåˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œåˆ¶è¡¨ç¬¦:{tab_count} vs é€—å·:{comma_count}ï¼‰")
            else:
                detected_type = "raw"
                print(f"ğŸ¯ è‡ªåŠ¨æ£€æµ‹ï¼šæ–‡ä»¶ç±»å‹ä¸º rawï¼ˆé€—å·åˆ†éš”ï¼Œé€—å·:{comma_count} vs åˆ¶è¡¨ç¬¦:{tab_count}ï¼‰")
        
    except Exception as e:
        print(f"âš ï¸ æ–‡ä»¶ç±»å‹è‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ raw ç±»å‹: {e}")
        detected_type = "raw"
    
    print(f"ï¿½ æœ€ç»ˆä½¿ç”¨çš„æ–‡ä»¶ç±»å‹: {detected_type}")
    
    # æ ¹æ®æ£€æµ‹åˆ°çš„ç±»å‹å¤„ç†æ–‡ä»¶
    df = None
    if detected_type == "raw":
        try:
            print("ğŸ“– æŒ‰ raw ç±»å‹å¤„ç†æ–‡ä»¶")
            # å°è¯•å¤šç§ç¼–ç è¯»å–rawç±»å‹æ–‡ä»¶
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            for encoding in encodings:
                try:
                    print(f"å°è¯•ä½¿ç”¨ç¼–ç  {encoding} è¯»å– raw ç±»å‹æ–‡ä»¶")
                    df = pd.read_csv(file, index_col=0, header=0, sep=",", encoding=encoding)
                    print(f"âœ… rawç±»å‹æˆåŠŸä½¿ç”¨ç¼–ç : {encoding}")
                    break
                except UnicodeDecodeError as e:
                    print(f"ç¼–ç  {encoding} è¯»å–å¤±è´¥: {str(e)}")
                    continue
                except Exception as e:
                    print(f"ç¼–ç  {encoding} å¤„ç†å¼‚å¸¸: {str(e)}")
                    if encoding == encodings[-1]:  # æœ€åä¸€ä¸ªç¼–ç ä¹Ÿå¤±è´¥äº†
                        raise e
                    continue
            
            if df is None:
                raise Exception("æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å»æ‰æœ€å11è¡Œï¼ˆrawç±»å‹çš„ç‰¹å¾ï¼‰
            original_shape = df.shape
            if df.shape[0] > 11:
                df = df.iloc[:-11, :]
                print(f"åˆ é™¤æœ€å11è¡Œï¼Œæ•°æ®å½¢çŠ¶ä» {original_shape} å˜ä¸º {df.shape}")
            df = df.T
            print(f"è½¬ç½®åæ•°æ®å½¢çŠ¶: {df.shape}")
            df.index = df.index.str.replace("0.000000.1", "0")
            df.index = pd.to_numeric(df.index)
            df.columns = pd.to_numeric(df.columns)
            print("âœ… raw ç±»å‹æ–‡ä»¶å¤„ç†æˆåŠŸ")
        except Exception as e:
            # å¦‚æœrawç±»å‹å¤„ç†å¤±è´¥ï¼Œå°è¯•handleç±»å‹
            print(f"âš ï¸ æŒ‰rawç±»å‹å¤„ç†å¤±è´¥ï¼Œå°è¯•handleç±»å‹: {str(e)}")
            try:
                # å°è¯•å¤šç§ç¼–ç è¯»å–handleç±»å‹æ–‡ä»¶
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
                df = None
                for encoding in encodings:
                    try:
                        print(f"å°è¯•ä½¿ç”¨ç¼–ç  {encoding} è¯»å– handle ç±»å‹æ–‡ä»¶ï¼ˆè‡ªåŠ¨åˆ‡æ¢ï¼‰")
                        df = pd.read_csv(file, index_col=0, header=0, sep="\t", encoding=encoding)
                        print(f"âœ… handleç±»å‹ä½¿ç”¨ç¼–ç : {encoding}")
                        break
                    except UnicodeDecodeError as e:
                        print(f"ç¼–ç  {encoding} è¯»å–å¤±è´¥: {str(e)}")
                        continue
                    except Exception as e_inner:
                        print(f"ç¼–ç  {encoding} å¤„ç†å¼‚å¸¸: {str(e_inner)}")
                        if encoding == encodings[-1]:
                            raise e_inner
                        continue
                
                if df is None:
                    raise Exception("æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶")
                
                # handleç±»å‹æ–‡ä»¶ä¸éœ€è¦è½¬ç½®ï¼Œå› ä¸ºæ•°æ®æ’åˆ—å·²ç»æ˜¯æ­£ç¡®çš„ï¼ˆè¡Œ=æ—¶é—´ï¼Œåˆ—=æ³¢é•¿ï¼‰
                # å¤„ç†ç´¢å¼•ï¼šå¦‚æœç´¢å¼•æ˜¯å­—ç¬¦ä¸²ç±»å‹æ‰è¿›è¡Œæ›¿æ¢æ“ä½œ
                if df.index.dtype == 'object':
                    df.index = df.index.str.replace("0.000000000E+0.1", "0")
                    df.index = df.index.str.replace("0.00000E+0.1", "0")
                # ç¡®ä¿ç´¢å¼•å’Œåˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
                try:
                    df.index = pd.to_numeric(df.index)
                except (ValueError, TypeError):
                    print("âš ï¸ ç´¢å¼•è½¬ä¸ºæ•°å€¼ç±»å‹å¤±è´¥ï¼Œä¿æŒåŸæ ·")
                df.columns = pd.to_numeric(df.columns)
                detected_type = "handle"
                print("âœ… è‡ªåŠ¨åˆ‡æ¢åˆ°handleç±»å‹å¤„ç†æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°è¯•äº†rawå’Œhandleä¸¤ç§æ ¼å¼éƒ½æ— æ³•æ­£ç¡®è§£æï¼š\nrawæ ¼å¼é”™è¯¯: {e}\nhandleæ ¼å¼é”™è¯¯: {e2}")
                raise Exception(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°è¯•äº†rawå’Œhandleä¸¤ç§æ ¼å¼éƒ½æ— æ³•æ­£ç¡®è§£æï¼š\nrawæ ¼å¼é”™è¯¯: {e}\nhandleæ ¼å¼é”™è¯¯: {e2}")
                
    elif detected_type == "handle":
        try:
            print("ğŸ“– æŒ‰ handle ç±»å‹å¤„ç†æ–‡ä»¶")
            # å°è¯•å¤šç§ç¼–ç è¯»å–handleç±»å‹æ–‡ä»¶
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            for encoding in encodings:
                try:
                    print(f"å°è¯•ä½¿ç”¨ç¼–ç  {encoding} è¯»å– handle ç±»å‹æ–‡ä»¶")
                    df = pd.read_csv(file, index_col=0, header=0, sep="\t", encoding=encoding)
                    print(f"âœ… handleç±»å‹æˆåŠŸä½¿ç”¨ç¼–ç : {encoding}")
                    break
                except UnicodeDecodeError as e:
                    print(f"ç¼–ç  {encoding} è¯»å–å¤±è´¥: {str(e)}")
                    continue
                except Exception as e_inner:
                    print(f"ç¼–ç  {encoding} å¤„ç†å¼‚å¸¸: {str(e_inner)}")
                    if encoding == encodings[-1]:
                        raise e_inner
                    continue
            
            if df is None:
                raise Exception("æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶")
            
            # handleç±»å‹æ–‡ä»¶ä¸éœ€è¦è½¬ç½®ï¼Œå› ä¸ºæ•°æ®æ’åˆ—å·²ç»æ˜¯æ­£ç¡®çš„ï¼ˆè¡Œ=æ—¶é—´ï¼Œåˆ—=æ³¢é•¿ï¼‰
            # å¤„ç†ç´¢å¼•ï¼šå¦‚æœç´¢å¼•æ˜¯å­—ç¬¦ä¸²ç±»å‹æ‰è¿›è¡Œæ›¿æ¢æ“ä½œ
            if df.index.dtype == 'object':
                df.index = df.index.str.replace("0.000000000E+0.1", "0")  # å°†æ•°æ®0ç‚¹æ”¹ä¸º0
                df.index = df.index.str.replace("0.00000E+0.1", "0")  # bugæš‚ä¿®ï¼Œåç»­ä¿®å¤
            # ç¡®ä¿ç´¢å¼•å’Œåˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
            try:
                df.index = pd.to_numeric(df.index)
            except (ValueError, TypeError):
                print("âš ï¸ ç´¢å¼•è½¬ä¸ºæ•°å€¼ç±»å‹å¤±è´¥ï¼Œä¿æŒåŸæ ·")
            df.columns = pd.to_numeric(df.columns)
            print("âœ… handle ç±»å‹æ–‡ä»¶å¤„ç†æˆåŠŸ")
        except Exception as e:
            print(f"âŒ handleç±»å‹æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            raise Exception(f"handleç±»å‹æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    # å¤„ç†æ— ç©·å€¼
    if inf_handle:
        print("ğŸ”§ å¤„ç†æ— ç©·å€¼å’ŒNaNå€¼...")
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.ffill(axis=0)  # ä½¿ç”¨ç°ä»£çš„å¡«å……æ–¹æ³•
        print("âœ… æ— ç©·å€¼å¤„ç†å®Œæˆ")
    
    # è®¾ç½®é»˜è®¤çš„æ³¢é•¿å’Œæ—¶é—´èŒƒå›´
    if wavelength_range is None:
        wavelength_range = [None, None]
    if delay_range is None:
        delay_range = [None, None]
        
    # é€‰æ‹©æ³¢é•¿èŒƒå›´å’Œæ—¶é—´èŒƒå›´
    if wavelength_range[0] is not None or wavelength_range[1] is not None:
        print(f"ğŸ” ç­›é€‰æ³¢é•¿èŒƒå›´: {wavelength_range}")
        df = select_wavelength(df, wavelength_range)
        
    if delay_range[0] is not None or delay_range[1] is not None:
        print(f"ğŸ” ç­›é€‰å»¶è¿Ÿæ—¶é—´èŒƒå›´: {delay_range}")
        df = select_delay(df, delay_range)
    
    # æ ‡ç­¾å°æ•°ç‚¹åä¿ç•™ä¸¤ä½
    df.index = df.index.map(lambda x: round(x, 2))
    df.columns = df.columns.map(lambda x: round(x, 2))
    
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ•°æ®å½¢çŠ¶: {df.shape}")
    return df

def select_wavelength(df, wavelength_range):
    """é€‰æ‹©æ³¢é•¿èŒƒå›´"""
    if wavelength_range[0] is None:
        wavelength_range[0] = df.columns.min()
    if wavelength_range[1] is None:
        wavelength_range[1] = df.columns.max()
    df = df.loc[:, (df.columns >= wavelength_range[0]) &
                (df.columns <= wavelength_range[1])]
    return df

def select_delay(df, delay_range):
    """é€‰æ‹©å»¶è¿Ÿæ—¶é—´èŒƒå›´"""
    if delay_range[0] is None:
        delay_range[0] = df.index.min()
    if delay_range[1] is None:
        delay_range[1] = df.index.max()
    df = df.loc[(df.index >= delay_range[0]) &
                (df.index <= delay_range[1]), :]
    return df

def load_real_tas_data():
    """åŠ è½½çœŸå®çš„TASæ•°æ®ï¼ˆä½¿ç”¨è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼‰"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = Path("D:/TAS/tas_mcr_als_project/data/TAS/TA_Average.csv")
    
    if not data_file.exists():
        # å¦‚æœç»å¯¹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ç›¸å¯¹è·¯å¾„
        data_file = Path("data/TAS/TA_Average.csv")
        
    if not data_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_file}")
        return None
    
    print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
    
    try:
        # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„æ•°æ®è¯»å–å‡½æ•°
        df = read_file_auto(
            str(data_file), 
            inf_handle=True,
            wavelength_range=(400, 800),  # 400-800nm
            delay_range=(0, 100)  # 0-100ps
        )
        
        return df
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def analyze_data_quality(data):
    """åˆ†ææ•°æ®è´¨é‡"""
    print("\n" + "="*50)
    print("ğŸ“Š æ•°æ®è´¨é‡åˆ†æ")
    print("="*50)
    
    print(f"æ•°æ®ç»´åº¦: {data.shape}")
    print(f"å»¶è¿Ÿæ—¶é—´èŒƒå›´: {data.index.min():.2f} - {data.index.max():.2f}")
    print(f"æ³¢é•¿èŒƒå›´: {data.columns.min():.1f} - {data.columns.max():.1f} nm")
    print(f"æ•°æ®èŒƒå›´: {data.values.min():.6f} - {data.values.max():.6f}")
    print(f"æ•°æ®å‡å€¼: {data.values.mean():.6f}")
    print(f"æ•°æ®æ ‡å‡†å·®: {data.values.std():.6f}")
    
    # æ£€æŸ¥NaNå’Œæ— ç©·å€¼
    nan_count = np.isnan(data.values).sum()
    inf_count = np.isinf(data.values).sum()
    print(f"NaNå€¼æ•°é‡: {nan_count}")
    print(f"æ— ç©·å€¼æ•°é‡: {inf_count}")
    
    # ä¼°ç®—ä¿¡å™ªæ¯”
    # ä½¿ç”¨é«˜é¢‘æˆåˆ†ä¼°ç®—å™ªå£°
    if data.shape[1] > 4:
        noise_estimate = np.std(np.diff(data.values, n=2, axis=1)) / np.sqrt(6)
        signal_estimate = np.std(data.values)
        snr_estimate = signal_estimate / noise_estimate if noise_estimate > 0 else float('inf')
        print(f"ä¼°ç®—ä¿¡å™ªæ¯”: {snr_estimate:.2f}")
    
def test_preprocessing_pipelines(data):
    """æµ‹è¯•ä¸åŒçš„é¢„å¤„ç†ç®¡é“"""
    print("\n" + "="*50)
    print("ğŸ”§ é¢„å¤„ç†ç®¡é“æµ‹è¯•")
    print("="*50)
    
    results = {}
    
    # 1. æ ‡å‡†ç®¡é“
    print("\n1. æ ‡å‡†é¢„å¤„ç†ç®¡é“")
    try:
        pipeline_std = create_standard_pipeline(verbose=True)
        processed_std = pipeline_std.fit_transform(data.copy())
        results['standard'] = {
            'data': processed_std,
            'pipeline': pipeline_std,
            'name': 'æ ‡å‡†ç®¡é“'
        }
        print("âœ… æ ‡å‡†ç®¡é“å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ ‡å‡†ç®¡é“å¤±è´¥: {e}")
        results['standard'] = None
    
    # 2. æ¸©å’Œç®¡é“
    print("\n2. æ¸©å’Œé¢„å¤„ç†ç®¡é“")
    try:
        pipeline_gentle = create_gentle_pipeline(verbose=False)
        processed_gentle = pipeline_gentle.fit_transform(data.copy())
        results['gentle'] = {
            'data': processed_gentle,
            'pipeline': pipeline_gentle,
            'name': 'æ¸©å’Œç®¡é“'
        }
        print("âœ… æ¸©å’Œç®¡é“å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¸©å’Œç®¡é“å¤±è´¥: {e}")
        results['gentle'] = None
    
    # 3. æ¿€è¿›ç®¡é“
    print("\n3. æ¿€è¿›é¢„å¤„ç†ç®¡é“")
    try:
        pipeline_aggressive = create_aggressive_pipeline(verbose=False)
        processed_aggressive = pipeline_aggressive.fit_transform(data.copy())
        results['aggressive'] = {
            'data': processed_aggressive,
            'pipeline': pipeline_aggressive,
            'name': 'æ¿€è¿›ç®¡é“'
        }
        print("âœ… æ¿€è¿›ç®¡é“å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¿€è¿›ç®¡é“å¤±è´¥: {e}")
        results['aggressive'] = None
    
    # 4. è‡ªå®šä¹‰ç®¡é“ï¼ˆé’ˆå¯¹TASæ•°æ®ä¼˜åŒ–ï¼‰
    print("\n4. TASä¼˜åŒ–ç®¡é“")
    try:
        tas_optimized_steps = [
            {'name': 'outlier_detection', 'processor': 'outlier', 
             'params': {'method': 'z_score', 'threshold': 2.5}, 'strategy': 'interpolate'},
            {'name': 'baseline_correction', 'processor': 'baseline', 
             'params': {'method': 'als', 'lam': 1e7, 'p': 0.001}},
            {'name': 'noise_filtering', 'processor': 'noise', 
             'params': {'method': 'gaussian', 'sigma': 0.8}},
            {'name': 'data_smoothing', 'processor': 'smooth', 
             'params': {'method': 'savgol', 'window_length': 7, 'polyorder': 3}}
        ]
        
        pipeline_tas = TASPreprocessingPipeline(steps=tas_optimized_steps, verbose=False)
        processed_tas = pipeline_tas.fit_transform(data.copy())
        results['tas_optimized'] = {
            'data': processed_tas,
            'pipeline': pipeline_tas,
            'name': 'TASä¼˜åŒ–ç®¡é“'
        }
        print("âœ… TASä¼˜åŒ–ç®¡é“å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ TASä¼˜åŒ–ç®¡é“å¤±è´¥: {e}")
        results['tas_optimized'] = None
    
    return results

def create_comprehensive_visualization(original_data, processed_results):
    """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–å¯¹æ¯”"""
    print("\n" + "="*50)
    print("ğŸ“ˆ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨")
    print("="*50)
    
    # å‡†å¤‡æ•°æ®
    original_wavelengths = np.asarray(original_data.columns.values, dtype=float)
    original_delays = np.asarray(original_data.index.values, dtype=float)

    def ensure_axis_length(candidates, expected_length):
        """ä»å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©ä¸æ•°æ®é•¿åº¦åŒ¹é…çš„è½´ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™å›é€€ç¬¬ä¸€ä¸ªå¯ç”¨é€‰é¡¹"""
        for axis in candidates:
            if axis is None:
                continue
            axis_array = np.asarray(axis, dtype=float)
            if axis_array.size == expected_length:
                return axis_array
        for axis in candidates:
            if axis is None:
                continue
            axis_array = np.asarray(axis, dtype=float)
            if axis_array.size:
                return axis_array
        return np.arange(expected_length, dtype=float)

    def extract_result_entry(key, result):
        """ç»Ÿä¸€æå–ç»“æœä¸­çš„æ•°æ®ã€åç§°å’Œåæ ‡è½´ä¿¡æ¯"""
        if result is None:
            return None

        name = key
        pipeline = None
        data_obj = result

        if isinstance(result, dict):
            data_obj = result.get('data')
            name = result.get('name', key)
            pipeline = result.get('pipeline')

        if data_obj is None:
            return None

        data_values = data_obj.values if hasattr(data_obj, 'values') else np.asarray(data_obj)

        if data_values.ndim == 1:
            data_values = data_values.reshape(1, -1)

        wl_candidates = []
        delay_candidates = []

        if hasattr(data_obj, 'columns'):
            wl_candidates.append(data_obj.columns.values)
        if pipeline is not None and getattr(pipeline, 'wavelengths', None) is not None:
            wl_candidates.append(pipeline.wavelengths)
        wl_candidates.append(original_wavelengths)

        if hasattr(data_obj, 'index'):
            delay_candidates.append(data_obj.index.values)
        if pipeline is not None and getattr(pipeline, 'time_axis', None) is not None:
            delay_candidates.append(pipeline.time_axis)
        delay_candidates.append(original_delays)

        wavelengths = ensure_axis_length(wl_candidates, data_values.shape[1])
        delays = ensure_axis_length(delay_candidates, data_values.shape[0])

        return {
            'key': key,
            'name': name,
            'data': data_obj,
            'values': np.asarray(data_values, dtype=float),
            'wavelengths': np.asarray(wavelengths, dtype=float),
            'delays': np.asarray(delays, dtype=float),
            'pipeline': pipeline
        }

    def find_nearest_index(axis_values, target_value):
        if axis_values.size == 0:
            return 0
        idx = int(np.argmin(np.abs(axis_values - target_value)))
        return max(0, min(idx, axis_values.size - 1))

    processed_entries = []
    for key, res in processed_results.items():
        entry = extract_result_entry(key, res)
        if entry is not None:
            processed_entries.append(entry)
    
    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§çš„å»¶è¿Ÿæ—¶é—´
    delay_indices = [
        len(original_delays) // 8,      # æ—©æœŸæ—¶é—´
        len(original_delays) // 4,      # æ—©æœŸ-ä¸­æœŸ
        len(original_delays) // 2,      # ä¸­æœŸ
        3 * len(original_delays) // 4,  # ä¸­æœŸ-åæœŸ
        -2                              # åæœŸ
    ]
    
    # 1. 2Dçƒ­å›¾å¯¹æ¯”
    print("ç”Ÿæˆ2Dçƒ­å›¾å¯¹æ¯”...")
    n_plots = 1 + len(processed_entries)

    fig1, axes = plt.subplots(1, min(n_plots, 4), figsize=(20, 5))
    if isinstance(axes, np.ndarray):
        axes_list = list(np.atleast_1d(axes))
    else:
        axes_list = [axes]

    all_arrays = [original_data.values]
    for entry in processed_entries:
        all_arrays.append(entry['values'])

    abs_max = max(np.nanmax(np.abs(arr)) for arr in all_arrays if arr.size) if all_arrays else 0.0
    if not np.isfinite(abs_max) or abs_max == 0:
        abs_max = 1.0

    norm = TwoSlopeNorm(vcenter=0.0, vmin=-abs_max, vmax=abs_max)
    cmap = plt.get_cmap('rainbow')
    
    # åŸå§‹æ•°æ®
    im0 = axes_list[0].imshow(original_data.values, aspect='auto', origin='lower',
                        extent=[original_wavelengths[0], original_wavelengths[-1], original_delays[0], original_delays[-1]],
                        cmap=cmap, norm=norm)
    axes_list[0].set_title('åŸå§‹æ•°æ®', fontsize=12, fontweight='bold')
    axes_list[0].set_xlabel('æ³¢é•¿ (nm)')
    axes_list[0].set_ylabel('å»¶è¿Ÿæ—¶é—´ (ps)')
    plt.colorbar(im0, ax=axes_list[0], shrink=0.8)
    
    # å¤„ç†åæ•°æ®
    plot_idx = 1
    for entry in processed_entries:
        if plot_idx >= len(axes_list):
            break

        im = axes_list[plot_idx].imshow(
            entry['values'],
            aspect='auto',
            origin='lower',
            extent=[
                entry['wavelengths'][0],
                entry['wavelengths'][-1],
                entry['delays'][0],
                entry['delays'][-1]
            ],
            cmap=cmap,
            norm=norm
        )
        axes_list[plot_idx].set_title(entry['name'], fontsize=12, fontweight='bold')
        axes_list[plot_idx].set_xlabel('æ³¢é•¿ (nm)')
        if plot_idx == 0:
            axes_list[plot_idx].set_ylabel('å»¶è¿Ÿæ—¶é—´ (ps)')
        plt.colorbar(im, ax=axes_list[plot_idx], shrink=0.8)
        plot_idx += 1
    
    plt.tight_layout()
    plt.savefig('tas_preprocessing_heatmap_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ… ä¿å­˜çƒ­å›¾å¯¹æ¯”: tas_preprocessing_heatmap_comparison.png")
    
    # 2. ä¸åŒæ—¶é—´ç‚¹çš„å…‰è°±å¯¹æ¯”
    print("ç”Ÿæˆå…‰è°±å¯¹æ¯”å›¾...")
    fig2, axes = plt.subplots(len(delay_indices), 1, figsize=(12, 3*len(delay_indices)))
    if len(delay_indices) == 1:
        axes = [axes]
    
    colors = ['black', 'red', 'blue', 'green', 'orange']
    
    for i, delay_idx in enumerate(delay_indices):
        delay_time = original_delays[delay_idx]
        
        # åŸå§‹æ•°æ®
        axes[i].plot(original_wavelengths, original_data.iloc[delay_idx], 
                    color='black', linewidth=2, alpha=0.7, label='åŸå§‹æ•°æ®')
        
        # å¤„ç†åæ•°æ®
        color_idx = 1
        for entry in processed_entries:
            if color_idx >= len(colors):
                break

            dataset_delays = entry['delays']
            target_idx = find_nearest_index(dataset_delays, delay_time)
            data_slice = entry['values'][target_idx]

            axes[i].plot(
                entry['wavelengths'],
                data_slice,
                color=colors[color_idx],
                linewidth=1.5,
                alpha=0.8,
                label=entry['name']
            )
            color_idx += 1
        
        axes[i].set_title(f'å»¶è¿Ÿæ—¶é—´: {delay_time:.2f} ps', fontsize=12, fontweight='bold')
        axes[i].set_xlabel('æ³¢é•¿ (nm)')
        axes[i].set_ylabel('Î”OD')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('tas_preprocessing_spectra_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ… ä¿å­˜å…‰è°±å¯¹æ¯”: tas_preprocessing_spectra_comparison.png")
    
    # 3. åŠ¨åŠ›å­¦æ›²çº¿å¯¹æ¯”
    print("ç”ŸæˆåŠ¨åŠ›å­¦æ›²çº¿å¯¹æ¯”...")
    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ³¢é•¿
    wavelength_indices = [
        len(original_wavelengths) // 4,
        len(original_wavelengths) // 2,
        3 * len(original_wavelengths) // 4
    ]
    wavelength_indices = [min(max(idx, 0), len(original_wavelengths) - 1) for idx in wavelength_indices]

    target_wavelengths = original_wavelengths[wavelength_indices]

    fig3, axes = plt.subplots(len(wavelength_indices), 1, figsize=(12, 3*len(wavelength_indices)))
    if len(wavelength_indices) == 1:
        axes = [axes]
    
    for i, wl_idx in enumerate(wavelength_indices):
        wavelength = original_wavelengths[wl_idx]
        
        # åŸå§‹æ•°æ®
        axes[i].semilogx(original_delays, original_data.iloc[:, wl_idx], 
                        color='black', linewidth=2, alpha=0.7, label='åŸå§‹æ•°æ®')
        
        # å¤„ç†åæ•°æ®
        color_idx = 1
        for entry in processed_entries:
            if color_idx >= len(colors):
                break

            dataset_wavelengths = entry['wavelengths']
            nearest_idx = find_nearest_index(dataset_wavelengths, target_wavelengths[i])
            data_slice = entry['values'][:, nearest_idx]

            axes[i].semilogx(
                entry['delays'],
                data_slice,
                color=colors[color_idx],
                linewidth=1.5,
                alpha=0.8,
                label=f"{entry['name']} ({dataset_wavelengths[nearest_idx]:.1f} nm)"
            )
            color_idx += 1
        
        axes[i].set_title(f'æ³¢é•¿: {wavelength:.1f} nm', fontsize=12, fontweight='bold')
        axes[i].set_xlabel('å»¶è¿Ÿæ—¶é—´ (ps)')
        axes[i].set_ylabel('Î”OD')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('tas_preprocessing_kinetics_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ… ä¿å­˜åŠ¨åŠ›å­¦å¯¹æ¯”: tas_preprocessing_kinetics_comparison.png")
    
    # 4. ç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”
    print("ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”...")
    fig4, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æ•°æ®èŒƒå›´å¯¹æ¯”
    names = ['åŸå§‹æ•°æ®']
    data_ranges = [(original_data.values.min(), original_data.values.max())]
    stds = [original_data.values.std()]
    snrs = []
    
    # ä¼°ç®—åŸå§‹æ•°æ®SNR
    try:
        noise_orig = np.std(np.diff(original_data.values, n=2, axis=1)) / np.sqrt(6)
        signal_orig = np.std(original_data.values)
        snr_orig = signal_orig / noise_orig if noise_orig > 0 else 0
        snrs.append(snr_orig)
    except:
        snrs.append(0)
    
    for key, result in processed_results.items():
        if result is not None:
            # æ£€æŸ¥resultæ˜¯å¦ä¸ºå­—å…¸æ ¼å¼ï¼Œå¦‚æœä¸æ˜¯åˆ™ç›´æ¥ä½¿ç”¨
            if isinstance(result, dict) and 'data' in result:
                data = result['data']
                name = result.get('name', key)
            else:
                data = result  # ç›´æ¥ä½¿ç”¨æ•°æ®
                name = key
                
            names.append(name)
            
            # å¤„ç†æ•°æ®ç±»å‹å·®å¼‚
            if hasattr(data, 'values'):
                data_values = data.values
            else:
                data_values = data  # å‡è®¾å·²ç»æ˜¯numpyæ•°ç»„
            
            data_ranges.append((data_values.min(), data_values.max()))
            stds.append(data_values.std())
            
            # ä¼°ç®—SNR
            try:
                noise = np.std(np.diff(data_values, n=2, axis=1)) / np.sqrt(6)
                signal = np.std(data_values)
                snr = signal / noise if noise > 0 else 0
                snrs.append(snr)
            except:
                snrs.append(0)
    
    # æ•°æ®èŒƒå›´
    mins, maxs = zip(*data_ranges)
    x_pos = np.arange(len(names))
    
    axes[0, 0].bar(x_pos, mins, alpha=0.7, label='æœ€å°å€¼')
    axes[0, 0].bar(x_pos, maxs, alpha=0.7, label='æœ€å¤§å€¼')
    axes[0, 0].set_title('æ•°æ®èŒƒå›´å¯¹æ¯”')
    axes[0, 0].set_ylabel('Î”OD')
    axes[0, 0].set_xticks(x_pos)
    axes[0, 0].set_xticklabels(names, rotation=45, ha='right')
    axes[0, 0].legend()
    
    # æ ‡å‡†å·®å¯¹æ¯”
    axes[0, 1].bar(x_pos, stds, alpha=0.7, color='orange')
    axes[0, 1].set_title('æ•°æ®æ ‡å‡†å·®å¯¹æ¯”')
    axes[0, 1].set_ylabel('æ ‡å‡†å·®')
    axes[0, 1].set_xticks(x_pos)
    axes[0, 1].set_xticklabels(names, rotation=45, ha='right')
    
    # SNRå¯¹æ¯”
    axes[1, 0].bar(x_pos, snrs, alpha=0.7, color='green')
    axes[1, 0].set_title('ä¿¡å™ªæ¯”å¯¹æ¯”')
    axes[1, 0].set_ylabel('SNR')
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels(names, rotation=45, ha='right')
    
    # å¤„ç†æ—¶é—´å¯¹æ¯”
    processing_times = [0]  # åŸå§‹æ•°æ®æ— å¤„ç†æ—¶é—´
    for key, result in processed_results.items():
        if result is not None:
            # æ£€æŸ¥resultæ˜¯å¦ä¸ºå­—å…¸æ ¼å¼ä¸”åŒ…å«pipelineä¿¡æ¯
            if isinstance(result, dict) and 'pipeline' in result:
                summary = result['pipeline'].get_processing_summary()
                processing_times.append(summary.get('total_processing_time', 0))
            else:
                processing_times.append(0)  # æ— æ³•è·å–å¤„ç†æ—¶é—´ä¿¡æ¯
    
    axes[1, 1].bar(x_pos, processing_times, alpha=0.7, color='red')
    axes[1, 1].set_title('å¤„ç†æ—¶é—´å¯¹æ¯”')
    axes[1, 1].set_ylabel('æ—¶é—´ (ç§’)')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels(names, rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig('tas_preprocessing_statistics_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ… ä¿å­˜ç»Ÿè®¡å¯¹æ¯”: tas_preprocessing_statistics_comparison.png")

def print_processing_summaries(processed_results):
    """æ‰“å°å¤„ç†æ‘˜è¦"""
    print("\n" + "="*50)
    print("ğŸ“‹ é¢„å¤„ç†æ‘˜è¦æŠ¥å‘Š")
    print("="*50)
    
    for key, result in processed_results.items():
        if result is not None:
            # æ£€æŸ¥resultæ˜¯å¦ä¸ºå­—å…¸æ ¼å¼ï¼Œå¦‚æœä¸æ˜¯åˆ™è·³è¿‡æ‘˜è¦æ‰“å°
            if isinstance(result, dict) and 'name' in result and 'pipeline' in result:
                print(f"\nğŸ”§ {result['name']}:")
                
                summary = result['pipeline'].get_processing_summary()
                
                print(f"   å¤„ç†æ­¥éª¤æ•°: {summary.get('total_steps', 0)}")
                print(f"   æ€»å¤„ç†æ—¶é—´: {summary.get('total_processing_time', 0):.3f}s")
                
                if 'steps_summary' in summary:
                    for step in summary['steps_summary']:
                        print(f"   - {step['name']}: {step['time']:.3f}s")
                
                if 'data_statistics' in summary:
                    stats = summary['data_statistics']
                    print(f"   æ•°æ®æ”¹è¿›:")
                    print(f"     - ä¿¡å™ªæ¯”æå‡: {stats.get('snr_improvement', 1):.2f}x")
                    print(f"     - å™ªå£°é™ä½: {stats.get('noise_reduction', 0)*100:.1f}%")
                    print(f"     - æ•°æ®ä¿çœŸåº¦: {stats.get('data_preservation', 0):.3f}")
            else:
                print(f"\nğŸ”§ {key}: å¤„ç†å®Œæˆ (æ•°æ®å½¢çŠ¶: {result.shape if hasattr(result, 'shape') else 'N/A'})")

def test_chirp_correction(data, time_delays):
    """æµ‹è¯•å•å•¾æ ¡æ­£åŠŸèƒ½"""
    print("\n" + "="*50)
    print("âš¡ å•å•¾æ ¡æ­£æµ‹è¯•")
    print("="*50)
    
    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
    if isinstance(data, pd.DataFrame):
        data_array = data.values
        wavelengths = data.index.values if hasattr(data.index, 'values') else np.arange(data.shape[0])
    else:
        data_array = data
        wavelengths = np.arange(data.shape[0])
    
    print(f"æ•°æ®å½¢çŠ¶: {data_array.shape}")
    print(f"æ³¢é•¿èŒƒå›´: {wavelengths.min():.1f} - {wavelengths.max():.1f} nm")
    print(f"æ—¶é—´å»¶è¿ŸèŒƒå›´: {time_delays.min():.2f} - {time_delays.max():.2f} ps")
    
    # åˆå§‹åŒ–å•å•¾æ ¡æ­£å™¨
    chirp_corrector = ChirpCorrector(method='cross_correlation')
    
    results = {}
    
    # æµ‹è¯•ä¸åŒçš„å•å•¾æ ¡æ­£æ–¹æ³•
    methods_to_test = ['cross_correlation', 'solvent_response', 'polynomial']
    
    for method in methods_to_test:
        print(f"\nğŸ” æµ‹è¯•å•å•¾æ ¡æ­£æ–¹æ³•: {method}")
        try:
            # é‡æ–°åˆå§‹åŒ–æ ¡æ­£å™¨
            if method == 'solvent_response':
                corrector = ChirpCorrector(method=method, solvent_wavelengths=[400, 450])
            elif method == 'polynomial':
                corrector = ChirpCorrector(method=method, polynomial_order=3)
            else:
                corrector = ChirpCorrector(method=method)
            
            # æ‰§è¡Œå•å•¾æ ¡æ­£
            corrected_data = corrector.correct_chirp(data_array, time_delays)
            
            if corrected_data is not None:
                results[method] = {
                    'original': data_array,
                    'corrected': corrected_data,
                    'correction_stats': corrector.get_correction_stats() if hasattr(corrector, 'get_correction_stats') else {}
                }
                print(f"  âœ… {method} æ ¡æ­£æˆåŠŸ")
                print(f"     æ ¡æ­£å‰æ•°æ®èŒƒå›´: {data_array.min():.2e} - {data_array.max():.2e}")
                print(f"     æ ¡æ­£åæ•°æ®èŒƒå›´: {corrected_data.min():.2e} - {corrected_data.max():.2e}")
            else:
                print(f"  âŒ {method} æ ¡æ­£å¤±è´¥")
                
        except Exception as e:
            print(f"  âŒ {method} æ ¡æ­£å‡ºé”™: {str(e)}")
            continue
    
    # å¯è§†åŒ–å•å•¾æ ¡æ­£ç»“æœ
    if results:
        print(f"\nğŸ“Š ç”Ÿæˆå•å•¾æ ¡æ­£å¯¹æ¯”å¯è§†åŒ–...")
        visualize_chirp_correction_results(results, wavelengths, time_delays)
    
    return results

def visualize_chirp_correction_results(results, wavelengths, time_delays):
    """å¯è§†åŒ–å•å•¾æ ¡æ­£ç»“æœ"""
    n_methods = len(results)
    if n_methods == 0:
        return
    
    # åˆ›å»ºæ¯”è¾ƒå›¾
    fig, axes = plt.subplots(2, n_methods, figsize=(5*n_methods, 10))
    if n_methods == 1:
        axes = axes.reshape(2, 1)
    
    # é€‰æ‹©éƒ¨åˆ†æ³¢é•¿è¿›è¡Œå±•ç¤º
    wavelength_indices = np.linspace(0, len(wavelengths)-1, 5, dtype=int)
    selected_wavelengths = wavelengths[wavelength_indices]
    
    colors = plt.cm.viridis(np.linspace(0, 1, len(selected_wavelengths)))
    
    for i, (method, data_dict) in enumerate(results.items()):
        original = data_dict['original']
        corrected = data_dict['corrected']
        
        # ä¸Šæ–¹å­å›¾ï¼šæ ¡æ­£å‰
        ax1 = axes[0, i]
        for j, wl_idx in enumerate(wavelength_indices):
            ax1.plot(time_delays, original[wl_idx, :], 
                    color=colors[j], alpha=0.7, linewidth=1,
                    label=f'{selected_wavelengths[j]:.0f} nm')
        
        ax1.set_title(f'{method} - æ ¡æ­£å‰', fontsize=12, fontweight='bold')
        ax1.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax1.set_ylabel('Î”OD')
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=8)
        
        # ä¸‹æ–¹å­å›¾ï¼šæ ¡æ­£å
        ax2 = axes[1, i]
        for j, wl_idx in enumerate(wavelength_indices):
            ax2.plot(time_delays, corrected[wl_idx, :], 
                    color=colors[j], alpha=0.7, linewidth=1,
                    label=f'{selected_wavelengths[j]:.0f} nm')
        
        ax2.set_title(f'{method} - æ ¡æ­£å', fontsize=12, fontweight='bold')
        ax2.set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        ax2.set_ylabel('Î”OD')
        ax2.grid(True, alpha=0.3)
        ax2.legend(fontsize=8)
        
        # è®¾ç½®ç›¸åŒçš„yè½´èŒƒå›´ä¾¿äºæ¯”è¾ƒ
        y_min = min(original.min(), corrected.min()) * 1.1
        y_max = max(original.max(), corrected.max()) * 1.1
        ax1.set_ylim(y_min, y_max)
        ax2.set_ylim(y_min, y_max)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    output_path = project_root / "tas_chirp_correction_comparison.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å•å•¾æ ¡æ­£å¯¹æ¯”å›¾ä¿å­˜è‡³: {output_path}")
    
    # åˆ›å»ºæ›´è¯¦ç»†çš„2Dçƒ­å›¾æ¯”è¾ƒ
    fig_2d, axes_2d = plt.subplots(1, n_methods*2, figsize=(4*n_methods*2, 6))
    if n_methods == 1:
        axes_2d = [axes_2d] if not isinstance(axes_2d, np.ndarray) else axes_2d
    
    heatmap_arrays = []
    for data_dict in results.values():
        if data_dict is None:
            continue
        heatmap_arrays.append(np.asarray(data_dict['original']))
        heatmap_arrays.append(np.asarray(data_dict['corrected']))

    abs_max = max(np.nanmax(np.abs(arr)) for arr in heatmap_arrays if arr.size) if heatmap_arrays else 1.0
    if abs_max == 0 or not np.isfinite(abs_max):
        abs_max = 1.0
    norm = TwoSlopeNorm(vcenter=0.0, vmin=-abs_max, vmax=abs_max)
    cmap = plt.get_cmap('rainbow')

    plot_idx = 0
    for method, data_dict in results.items():
        original = data_dict['original']
        corrected = data_dict['corrected']
        
        # é€‰æ‹©é€‚å½“çš„æ³¢é•¿å’Œæ—¶é—´èŒƒå›´è¿›è¡Œæ˜¾ç¤º
        wl_slice = slice(0, min(50, original.shape[0]))
        time_slice = slice(0, min(100, original.shape[1]))
        
        # åŸå§‹æ•°æ®çƒ­å›¾
        im1 = axes_2d[plot_idx].imshow(
            original[wl_slice, time_slice],
            aspect='auto',
            cmap=cmap,
            norm=norm,
            extent=[
                time_delays[time_slice].min(),
                time_delays[time_slice].max(),
                wavelengths[wl_slice].max(),
                wavelengths[wl_slice].min(),
            ],
        )
        axes_2d[plot_idx].set_title(f'{method} - åŸå§‹', fontweight='bold')
        axes_2d[plot_idx].set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        axes_2d[plot_idx].set_ylabel('æ³¢é•¿ (nm)')
        plt.colorbar(im1, ax=axes_2d[plot_idx], shrink=0.6)
        
        # æ ¡æ­£åæ•°æ®çƒ­å›¾
        im2 = axes_2d[plot_idx + 1].imshow(
            corrected[wl_slice, time_slice],
            aspect='auto',
            cmap=cmap,
            norm=norm,
            extent=[
                time_delays[time_slice].min(),
                time_delays[time_slice].max(),
                wavelengths[wl_slice].max(),
                wavelengths[wl_slice].min(),
            ],
        )
        axes_2d[plot_idx+1].set_title(f'{method} - æ ¡æ­£å', fontweight='bold')
        axes_2d[plot_idx+1].set_xlabel('æ—¶é—´å»¶è¿Ÿ (ps)')
        axes_2d[plot_idx+1].set_ylabel('æ³¢é•¿ (nm)')
        plt.colorbar(im2, ax=axes_2d[plot_idx+1], shrink=0.6)
        
        plot_idx += 2
    
    plt.tight_layout()
    
    # ä¿å­˜2Dçƒ­å›¾
    output_path_2d = project_root / "tas_chirp_correction_heatmap.png"
    plt.savefig(output_path_2d, dpi=300, bbox_inches='tight')
    print(f"âœ… å•å•¾æ ¡æ­£2Dçƒ­å›¾ä¿å­˜è‡³: {output_path_2d}")
    
    plt.close('all')

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ TASçœŸå®æ•°æ®é¢„å¤„ç†æµ‹è¯•")
    print("="*60)
    
    # 1. åŠ è½½çœŸå®æ•°æ®
    original_data = load_real_tas_data()
    if original_data is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {original_data.shape}")
    
    # 1.5. ç”Ÿæˆæ—¶é—´å»¶è¿Ÿæ•°ç»„ï¼ˆå‡è®¾æ•°æ®çš„åˆ—æ˜¯æ—¶é—´å»¶è¿Ÿï¼‰
    # è¿™é‡Œå‡è®¾æ—¶é—´å»¶è¿Ÿä»-1 ps åˆ° 1000 ps
    n_time_points = original_data.shape[1] if isinstance(original_data, pd.DataFrame) else original_data.shape[1]
    time_delays = np.linspace(-1, 1000, n_time_points)  # ps
    print(f"â° æ—¶é—´å»¶è¿ŸèŒƒå›´: {time_delays.min():.2f} - {time_delays.max():.2f} ps")
    
    # 2. æ•°æ®è´¨é‡åˆ†æ
    analyze_data_quality(original_data)
    
    # 3. æµ‹è¯•å•å•¾æ ¡æ­£åŠŸèƒ½
    try:
        chirp_results = test_chirp_correction(original_data, time_delays)
        print(f"âœ… å•å•¾æ ¡æ­£æµ‹è¯•å®Œæˆï¼Œæµ‹è¯•äº† {len(chirp_results)} ç§æ–¹æ³•")
    except Exception as e:
        print(f"âŒ å•å•¾æ ¡æ­£æµ‹è¯•å¤±è´¥: {str(e)}")
        chirp_results = {}
    
    # 4. æµ‹è¯•é¢„å¤„ç†ç®¡é“
    processed_results = test_preprocessing_pipelines(original_data)
    
    # 5. æµ‹è¯•åŒ…å«å•å•¾æ ¡æ­£çš„å®Œæ•´ç®¡é“
    print("\n" + "="*50)
    print("ğŸŒŸ å®Œæ•´é¢„å¤„ç†ç®¡é“æµ‹è¯•ï¼ˆåŒ…å«å•å•¾æ ¡æ­£ï¼‰")
    print("="*50)
    
    try:
        # æµ‹è¯•å•å•¾æ ¡æ­£ç®¡é“
        chirp_pipeline = create_chirp_corrected_pipeline(chirp_method='cross_correlation', verbose=True)
        processed_with_chirp = chirp_pipeline.fit_transform(original_data.copy())
        print("âœ… å•å•¾æ ¡æ­£ç®¡é“æµ‹è¯•æˆåŠŸ")
        
        # æµ‹è¯•ç»¼åˆç®¡é“
        comprehensive_pipeline = create_comprehensive_pipeline(chirp_method='cross_correlation', verbose=True)
        processed_comprehensive = comprehensive_pipeline.fit_transform(original_data.copy())
        print("âœ… ç»¼åˆé¢„å¤„ç†ç®¡é“æµ‹è¯•æˆåŠŸ")
        
        # å°†ç»“æœæ·»åŠ åˆ°å¤„ç†ç»“æœä¸­
        processed_results['chirp_corrected'] = processed_with_chirp
        processed_results['comprehensive'] = processed_comprehensive
        
    except Exception as e:
        print(f"âŒ å®Œæ•´ç®¡é“æµ‹è¯•å¤±è´¥: {str(e)}")
    
    # 6. åˆ›å»ºå¯è§†åŒ–
    create_comprehensive_visualization(original_data, processed_results)
    
    # 7. æ‰“å°æ‘˜è¦
    print_processing_summaries(processed_results)
    
    # 8. æ˜¾ç¤ºå›¾ç‰‡
    print(f"\nğŸ“Š å·²ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶:")
    print("   - tas_preprocessing_heatmap_comparison.png")
    print("   - tas_preprocessing_spectra_comparison.png") 
    print("   - tas_preprocessing_kinetics_comparison.png")
    print("   - tas_preprocessing_statistics_comparison.png")
    if chirp_results:
        print("   - tas_chirp_correction_comparison.png")
        print("   - tas_chirp_correction_heatmap.png")
    
    # æ˜¾ç¤ºå›¾ç‰‡
    try:
        plt.show()
    except:
        print("\nğŸ’¡ è¯·æ‰‹åŠ¨æŸ¥çœ‹ç”Ÿæˆçš„PNGæ–‡ä»¶")
    
    print("\nğŸ‰ TASçœŸå®æ•°æ®é¢„å¤„ç†æµ‹è¯•å®Œæˆï¼")

if __name__ == '__main__':
    main()
